nohup: ignoring input
/root/miniconda3/envs/flux/lib/python3.13/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/root/miniconda3/envs/flux/lib/python3.13/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/root/miniconda3/envs/flux/lib/python3.13/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
05/07/2025 00:29:10 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: bf16

05/07/2025 00:29:11 - INFO - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at black-forest-labs/FLUX.1-dev.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
05/07/2025 00:29:11 - INFO - __main__ - loading vae
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 29676.68it/s]
{'out_channels', 'axes_dims_rope'} was not found in config. Values will be initialized to default values.
05/07/2025 00:29:12 - INFO - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.72s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.04s/it]
All model checkpoint weights were used when initializing FluxTransformer2DModel.

All the weights of FluxTransformer2DModel were initialized from the model checkpoint at black-forest-labs/FLUX.1-dev.
If your task is similar to the task the model of the checkpoint was trained on, you can already use FluxTransformer2DModel for predictions without further training.
05/07/2025 00:29:25 - INFO - __main__ - loading flux_transformer
05/07/2025 00:29:25 - INFO - __main__ - All models loaded successfully
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...:  20%|██        | 1/5 [00:00<00:00,  7.66it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...:  40%|████      | 2/5 [00:00<00:00,  6.72it/s]{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of black-forest-labs/FLUX.1-dev.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s][A
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s][ALoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]Loading pipeline components...: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s]
05/07/2025 00:29:28 - INFO - __main__ - Setting up custom tokens: ['<😊1>'] initialized with: ['sofa']
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
05/07/2025 00:29:28 - INFO - __main__ - Updating tokenizer and text encoder for the main pipeline
{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
05/07/2025 00:29:29 - INFO - __main__ - Number of trainable parameters: 18,677,760
05/07/2025 00:29:29 - INFO - __main__ - Percentage of trainable parameters: 0.16%
05/07/2025 00:29:29 - INFO - __main__ - ***** Running training *****
05/07/2025 00:29:29 - INFO - __main__ -   Num examples = 10
05/07/2025 00:29:29 - INFO - __main__ -   Num batches each epoch = 10
05/07/2025 00:29:29 - INFO - __main__ -   Num Epochs = 50
05/07/2025 00:29:29 - INFO - __main__ -   Instantaneous batch size per device = 1
05/07/2025 00:29:29 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1
05/07/2025 00:29:29 - INFO - __main__ -   Gradient Accumulation steps = 1
05/07/2025 00:29:29 - INFO - __main__ -   Total optimization steps = 500
Added 1 tokens to the tokenizer
Token IDs: [49408]
Initialized token ID 49408 with embedding of 'sofa'
Made 1 token embeddings learnable
Steps:   0%|          | 0/500 [00:00<?, ?it/s]Steps:   0%|          | 1/500 [00:08<1:10:12,  8.44s/it]Steps:   0%|          | 1/500 [00:08<1:10:12,  8.44s/it, loss=0.304, lr=0.0001]Steps:   0%|          | 2/500 [00:11<44:03,  5.31s/it, loss=0.304, lr=0.0001]  Steps:   0%|          | 2/500 [00:11<44:03,  5.31s/it, loss=0.113, lr=0.0001]Steps:   1%|          | 3/500 [00:14<35:54,  4.34s/it, loss=0.113, lr=0.0001]Steps:   1%|          | 3/500 [00:14<35:54,  4.34s/it, loss=0.505, lr=0.0001]Steps:   1%|          | 4/500 [00:17<32:05,  3.88s/it, loss=0.505, lr=0.0001]Steps:   1%|          | 4/500 [00:18<32:05,  3.88s/it, loss=0.371, lr=0.0001]Steps:   1%|          | 5/500 [00:21<29:54,  3.63s/it, loss=0.371, lr=0.0001]Steps:   1%|          | 5/500 [00:21<29:54,  3.63s/it, loss=0.435, lr=0.0001]Steps:   1%|          | 6/500 [00:24<28:35,  3.47s/it, loss=0.435, lr=0.0001]Steps:   1%|          | 6/500 [00:24<28:35,  3.47s/it, loss=0.0886, lr=0.0001]Steps:   1%|▏         | 7/500 [00:27<27:43,  3.37s/it, loss=0.0886, lr=0.0001]Steps:   1%|▏         | 7/500 [00:27<27:43,  3.37s/it, loss=0.515, lr=0.0001] Steps:   2%|▏         | 8/500 [00:30<27:09,  3.31s/it, loss=0.515, lr=0.0001]Steps:   2%|▏         | 8/500 [00:30<27:09,  3.31s/it, loss=0.108, lr=0.0001]Steps:   2%|▏         | 9/500 [00:33<26:58,  3.30s/it, loss=0.108, lr=0.0001]Steps:   2%|▏         | 9/500 [00:33<26:58,  3.30s/it, loss=0.369, lr=0.0001]Steps:   2%|▏         | 10/500 [00:37<26:33,  3.25s/it, loss=0.369, lr=0.0001]Steps:   2%|▏         | 10/500 [00:37<26:33,  3.25s/it, loss=0.198, lr=0.0001]Steps:   2%|▏         | 11/500 [00:40<26:23,  3.24s/it, loss=0.198, lr=0.0001]Steps:   2%|▏         | 11/500 [00:40<26:23,  3.24s/it, loss=0.18, lr=0.0001] Steps:   2%|▏         | 12/500 [00:43<26:11,  3.22s/it, loss=0.18, lr=0.0001]Steps:   2%|▏         | 12/500 [00:43<26:11,  3.22s/it, loss=0.427, lr=0.0001]Steps:   3%|▎         | 13/500 [00:46<26:01,  3.21s/it, loss=0.427, lr=0.0001]Steps:   3%|▎         | 13/500 [00:46<26:01,  3.21s/it, loss=0.101, lr=0.0001]Steps:   3%|▎         | 14/500 [00:49<25:54,  3.20s/it, loss=0.101, lr=0.0001]Steps:   3%|▎         | 14/500 [00:49<25:54,  3.20s/it, loss=0.854, lr=0.0001]Steps:   3%|▎         | 15/500 [00:52<25:46,  3.19s/it, loss=0.854, lr=0.0001]Steps:   3%|▎         | 15/500 [00:53<25:46,  3.19s/it, loss=0.491, lr=0.0001]Steps:   3%|▎         | 16/500 [00:56<25:43,  3.19s/it, loss=0.491, lr=0.0001]Steps:   3%|▎         | 16/500 [00:56<25:43,  3.19s/it, loss=0.119, lr=0.0001]Steps:   3%|▎         | 17/500 [00:59<25:38,  3.19s/it, loss=0.119, lr=0.0001]Steps:   3%|▎         | 17/500 [00:59<25:38,  3.19s/it, loss=0.506, lr=0.0001]Steps:   4%|▎         | 18/500 [01:02<26:26,  3.29s/it, loss=0.506, lr=0.0001]Steps:   4%|▎         | 18/500 [01:03<26:26,  3.29s/it, loss=0.267, lr=0.0001]Steps:   4%|▍         | 19/500 [01:06<27:22,  3.42s/it, loss=0.267, lr=0.0001]Steps:   4%|▍         | 19/500 [01:06<27:22,  3.42s/it, loss=0.0944, lr=0.0001]Steps:   4%|▍         | 20/500 [01:09<26:41,  3.34s/it, loss=0.0944, lr=0.0001]Steps:   4%|▍         | 20/500 [01:09<26:41,  3.34s/it, loss=0.193, lr=0.0001] Steps:   4%|▍         | 21/500 [01:12<26:18,  3.30s/it, loss=0.193, lr=0.0001]Steps:   4%|▍         | 21/500 [01:12<26:18,  3.30s/it, loss=0.508, lr=0.0001]Steps:   4%|▍         | 22/500 [01:16<25:58,  3.26s/it, loss=0.508, lr=0.0001]Steps:   4%|▍         | 22/500 [01:16<25:58,  3.26s/it, loss=0.198, lr=0.0001]Steps:   5%|▍         | 23/500 [01:19<25:42,  3.23s/it, loss=0.198, lr=0.0001]Steps:   5%|▍         | 23/500 [01:19<25:42,  3.23s/it, loss=0.46, lr=0.0001] Steps:   5%|▍         | 24/500 [01:22<25:30,  3.21s/it, loss=0.46, lr=0.0001]Steps:   5%|▍         | 24/500 [01:22<25:30,  3.21s/it, loss=0.523, lr=0.0001]Steps:   5%|▌         | 25/500 [01:25<25:20,  3.20s/it, loss=0.523, lr=0.0001]Steps:   5%|▌         | 25/500 [01:25<25:20,  3.20s/it, loss=0.175, lr=0.0001]Steps:   5%|▌         | 26/500 [01:28<25:12,  3.19s/it, loss=0.175, lr=0.0001]Steps:   5%|▌         | 26/500 [01:28<25:12,  3.19s/it, loss=0.2, lr=0.0001]  Steps:   5%|▌         | 27/500 [01:31<25:07,  3.19s/it, loss=0.2, lr=0.0001]Steps:   5%|▌         | 27/500 [01:32<25:07,  3.19s/it, loss=0.174, lr=0.0001]Steps:   6%|▌         | 28/500 [01:35<25:11,  3.20s/it, loss=0.174, lr=0.0001]Steps:   6%|▌         | 28/500 [01:35<25:11,  3.20s/it, loss=0.425, lr=0.0001]Steps:   6%|▌         | 29/500 [01:38<25:04,  3.20s/it, loss=0.425, lr=0.0001]Steps:   6%|▌         | 29/500 [01:38<25:04,  3.20s/it, loss=0.206, lr=0.0001]Steps:   6%|▌         | 30/500 [01:41<24:55,  3.18s/it, loss=0.206, lr=0.0001]Steps:   6%|▌         | 30/500 [01:41<24:55,  3.18s/it, loss=0.306, lr=0.0001]Steps:   6%|▌         | 31/500 [01:44<24:55,  3.19s/it, loss=0.306, lr=0.0001]Steps:   6%|▌         | 31/500 [01:44<24:55,  3.19s/it, loss=0.736, lr=0.0001]Steps:   6%|▋         | 32/500 [01:47<24:49,  3.18s/it, loss=0.736, lr=0.0001]Steps:   6%|▋         | 32/500 [01:47<24:49,  3.18s/it, loss=0.138, lr=0.0001]Steps:   7%|▋         | 33/500 [01:51<24:44,  3.18s/it, loss=0.138, lr=0.0001]Steps:   7%|▋         | 33/500 [01:51<24:44,  3.18s/it, loss=0.135, lr=0.0001]Steps:   7%|▋         | 34/500 [01:54<24:39,  3.18s/it, loss=0.135, lr=0.0001]Steps:   7%|▋         | 34/500 [01:54<24:39,  3.18s/it, loss=0.455, lr=0.0001]Steps:   7%|▋         | 35/500 [01:57<24:36,  3.17s/it, loss=0.455, lr=0.0001]Steps:   7%|▋         | 35/500 [01:57<24:36,  3.17s/it, loss=0.0925, lr=0.0001]Steps:   7%|▋         | 36/500 [02:00<24:33,  3.18s/it, loss=0.0925, lr=0.0001]Steps:   7%|▋         | 36/500 [02:00<24:33,  3.18s/it, loss=0.14, lr=0.0001]  Steps:   7%|▋         | 37/500 [02:03<24:30,  3.18s/it, loss=0.14, lr=0.0001]Steps:   7%|▋         | 37/500 [02:03<24:30,  3.18s/it, loss=0.25, lr=0.0001]Steps:   8%|▊         | 38/500 [02:06<24:27,  3.18s/it, loss=0.25, lr=0.0001]Steps:   8%|▊         | 38/500 [02:07<24:27,  3.18s/it, loss=0.282, lr=0.0001]Steps:   8%|▊         | 39/500 [02:10<24:23,  3.17s/it, loss=0.282, lr=0.0001]Steps:   8%|▊         | 39/500 [02:10<24:23,  3.17s/it, loss=0.489, lr=0.0001]Steps:   8%|▊         | 40/500 [02:13<24:16,  3.17s/it, loss=0.489, lr=0.0001]Steps:   8%|▊         | 40/500 [02:13<24:16,  3.17s/it, loss=1.81, lr=0.0001] Steps:   8%|▊         | 41/500 [02:16<24:18,  3.18s/it, loss=1.81, lr=0.0001]Steps:   8%|▊         | 41/500 [02:16<24:18,  3.18s/it, loss=0.452, lr=0.0001]Steps:   8%|▊         | 42/500 [02:19<24:14,  3.18s/it, loss=0.452, lr=0.0001]Steps:   8%|▊         | 42/500 [02:19<24:14,  3.18s/it, loss=1.05, lr=0.0001] Steps:   9%|▊         | 43/500 [02:22<24:10,  3.17s/it, loss=1.05, lr=0.0001]Steps:   9%|▊         | 43/500 [02:22<24:10,  3.17s/it, loss=0.115, lr=0.0001]Steps:   9%|▉         | 44/500 [02:25<24:08,  3.18s/it, loss=0.115, lr=0.0001]Steps:   9%|▉         | 44/500 [02:26<24:08,  3.18s/it, loss=0.153, lr=0.0001]Steps:   9%|▉         | 45/500 [02:29<24:04,  3.18s/it, loss=0.153, lr=0.0001]Steps:   9%|▉         | 45/500 [02:29<24:04,  3.18s/it, loss=0.48, lr=0.0001] Steps:   9%|▉         | 46/500 [02:32<24:01,  3.17s/it, loss=0.48, lr=0.0001]Steps:   9%|▉         | 46/500 [02:32<24:01,  3.17s/it, loss=0.144, lr=0.0001]Steps:   9%|▉         | 47/500 [02:35<23:57,  3.17s/it, loss=0.144, lr=0.0001]Steps:   9%|▉         | 47/500 [02:35<23:57,  3.17s/it, loss=0.508, lr=0.0001]Steps:  10%|▉         | 48/500 [02:38<23:54,  3.17s/it, loss=0.508, lr=0.0001]Steps:  10%|▉         | 48/500 [02:38<23:54,  3.17s/it, loss=0.0979, lr=0.0001]Steps:  10%|▉         | 49/500 [02:41<23:51,  3.17s/it, loss=0.0979, lr=0.0001]Steps:  10%|▉         | 49/500 [02:41<23:51,  3.17s/it, loss=0.488, lr=0.0001] Steps:  10%|█         | 50/500 [02:44<23:44,  3.17s/it, loss=0.488, lr=0.0001]05/07/2025 00:32:14 - INFO - __main__ - Running validation at step 50
05/07/2025 00:32:14 - INFO - __main__ - Running validation... 
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][AInstantiating AutoencoderKL model under default dtype torch.bfloat16.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at /root/autodl-tmp/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/vae.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
Loaded vae as AutoencoderKL from `vae` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.

Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 17.25it/s][A{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A[ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 69.91it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 30.48it/s]
05/07/2025 00:32:18 - INFO - __main__ - Loaded 6 prompts from validation_prompts.json
05/07/2025 00:33:00 - INFO - __main__ - Generated image 1/2 for prompt 1/6
05/07/2025 00:33:41 - INFO - __main__ - Generated image 2/2 for prompt 1/6
05/07/2025 00:34:22 - INFO - __main__ - Generated image 1/2 for prompt 2/6
05/07/2025 00:35:04 - INFO - __main__ - Generated image 2/2 for prompt 2/6
05/07/2025 00:35:45 - INFO - __main__ - Generated image 1/2 for prompt 3/6
05/07/2025 00:36:26 - INFO - __main__ - Generated image 2/2 for prompt 3/6
05/07/2025 00:37:08 - INFO - __main__ - Generated image 1/2 for prompt 4/6
05/07/2025 00:37:49 - INFO - __main__ - Generated image 2/2 for prompt 4/6
05/07/2025 00:38:31 - INFO - __main__ - Generated image 1/2 for prompt 5/6
05/07/2025 00:39:12 - INFO - __main__ - Generated image 2/2 for prompt 5/6
05/07/2025 00:39:53 - INFO - __main__ - Generated image 1/2 for prompt 6/6
05/07/2025 00:40:35 - INFO - __main__ - Generated image 2/2 for prompt 6/6
Steps:  10%|█         | 50/500 [11:05<23:44,  3.17s/it, loss=0.331, lr=0.0001]Steps:  10%|█         | 51/500 [11:08<19:07:10, 153.30s/it, loss=0.331, lr=0.0001]Steps:  10%|█         | 51/500 [11:08<19:07:10, 153.30s/it, loss=0.494, lr=0.0001]Steps:  10%|█         | 52/500 [11:11<13:28:18, 108.26s/it, loss=0.494, lr=0.0001]Steps:  10%|█         | 52/500 [11:11<13:28:18, 108.26s/it, loss=0.481, lr=0.0001]Steps:  11%|█         | 53/500 [11:14<9:31:37, 76.73s/it, loss=0.481, lr=0.0001]  Steps:  11%|█         | 53/500 [11:14<9:31:37, 76.73s/it, loss=0.106, lr=0.0001]Steps:  11%|█         | 54/500 [11:18<6:46:19, 54.66s/it, loss=0.106, lr=0.0001]Steps:  11%|█         | 54/500 [11:18<6:46:19, 54.66s/it, loss=0.183, lr=0.0001]Steps:  11%|█         | 55/500 [11:21<4:50:51, 39.22s/it, loss=0.183, lr=0.0001]Steps:  11%|█         | 55/500 [11:21<4:50:51, 39.22s/it, loss=0.252, lr=0.0001]Steps:  11%|█         | 56/500 [11:24<3:30:10, 28.40s/it, loss=0.252, lr=0.0001]Steps:  11%|█         | 56/500 [11:24<3:30:10, 28.40s/it, loss=0.427, lr=0.0001]Steps:  11%|█▏        | 57/500 [11:27<2:33:49, 20.83s/it, loss=0.427, lr=0.0001]Steps:  11%|█▏        | 57/500 [11:27<2:33:49, 20.83s/it, loss=0.193, lr=0.0001]Steps:  12%|█▏        | 58/500 [11:30<1:54:25, 15.53s/it, loss=0.193, lr=0.0001]Steps:  12%|█▏        | 58/500 [11:30<1:54:25, 15.53s/it, loss=0.5, lr=0.0001]  Steps:  12%|█▏        | 59/500 [11:33<1:26:55, 11.83s/it, loss=0.5, lr=0.0001]Steps:  12%|█▏        | 59/500 [11:34<1:26:55, 11.83s/it, loss=0.115, lr=0.0001]Steps:  12%|█▏        | 60/500 [11:37<1:07:37,  9.22s/it, loss=0.115, lr=0.0001]Steps:  12%|█▏        | 60/500 [11:37<1:07:37,  9.22s/it, loss=0.231, lr=0.0001]Steps:  12%|█▏        | 61/500 [11:40<54:16,  7.42s/it, loss=0.231, lr=0.0001]  Steps:  12%|█▏        | 61/500 [11:40<54:16,  7.42s/it, loss=0.389, lr=0.0001]Steps:  12%|█▏        | 62/500 [11:43<44:51,  6.14s/it, loss=0.389, lr=0.0001]Steps:  12%|█▏        | 62/500 [11:43<44:51,  6.14s/it, loss=0.257, lr=0.0001]Steps:  13%|█▎        | 63/500 [11:46<38:14,  5.25s/it, loss=0.257, lr=0.0001]Steps:  13%|█▎        | 63/500 [11:46<38:14,  5.25s/it, loss=0.282, lr=0.0001]Steps:  13%|█▎        | 64/500 [11:49<33:36,  4.63s/it, loss=0.282, lr=0.0001]Steps:  13%|█▎        | 64/500 [11:49<33:36,  4.63s/it, loss=0.175, lr=0.0001]Steps:  13%|█▎        | 65/500 [11:52<30:21,  4.19s/it, loss=0.175, lr=0.0001]Steps:  13%|█▎        | 65/500 [11:53<30:21,  4.19s/it, loss=0.119, lr=0.0001]Steps:  13%|█▎        | 66/500 [11:56<28:04,  3.88s/it, loss=0.119, lr=0.0001]Steps:  13%|█▎        | 66/500 [11:56<28:04,  3.88s/it, loss=0.177, lr=0.0001]Steps:  13%|█▎        | 67/500 [11:59<26:28,  3.67s/it, loss=0.177, lr=0.0001]Steps:  13%|█▎        | 67/500 [11:59<26:28,  3.67s/it, loss=0.0952, lr=0.0001]Steps:  14%|█▎        | 68/500 [12:02<25:20,  3.52s/it, loss=0.0952, lr=0.0001]Steps:  14%|█▎        | 68/500 [12:02<25:20,  3.52s/it, loss=0.349, lr=0.0001] Steps:  14%|█▍        | 69/500 [12:05<24:31,  3.41s/it, loss=0.349, lr=0.0001]Steps:  14%|█▍        | 69/500 [12:05<24:31,  3.41s/it, loss=0.3, lr=0.0001]  Steps:  14%|█▍        | 70/500 [12:08<23:53,  3.33s/it, loss=0.3, lr=0.0001]Steps:  14%|█▍        | 70/500 [12:08<23:53,  3.33s/it, loss=0.303, lr=0.0001]Steps:  14%|█▍        | 71/500 [12:11<23:34,  3.30s/it, loss=0.303, lr=0.0001]Steps:  14%|█▍        | 71/500 [12:12<23:34,  3.30s/it, loss=0.266, lr=0.0001]Steps:  14%|█▍        | 72/500 [12:15<23:14,  3.26s/it, loss=0.266, lr=0.0001]Steps:  14%|█▍        | 72/500 [12:15<23:14,  3.26s/it, loss=0.706, lr=0.0001]Steps:  15%|█▍        | 73/500 [12:18<23:08,  3.25s/it, loss=0.706, lr=0.0001]Steps:  15%|█▍        | 73/500 [12:18<23:08,  3.25s/it, loss=0.159, lr=0.0001]Steps:  15%|█▍        | 74/500 [12:21<22:54,  3.23s/it, loss=0.159, lr=0.0001]Steps:  15%|█▍        | 74/500 [12:21<22:54,  3.23s/it, loss=0.188, lr=0.0001]Steps:  15%|█▌        | 75/500 [12:24<22:43,  3.21s/it, loss=0.188, lr=0.0001]Steps:  15%|█▌        | 75/500 [12:24<22:43,  3.21s/it, loss=0.163, lr=0.0001]Steps:  15%|█▌        | 76/500 [12:27<22:35,  3.20s/it, loss=0.163, lr=0.0001]Steps:  15%|█▌        | 76/500 [12:28<22:35,  3.20s/it, loss=0.196, lr=0.0001]Steps:  15%|█▌        | 77/500 [12:31<22:28,  3.19s/it, loss=0.196, lr=0.0001]Steps:  15%|█▌        | 77/500 [12:31<22:28,  3.19s/it, loss=0.254, lr=0.0001]Steps:  16%|█▌        | 78/500 [12:34<22:23,  3.18s/it, loss=0.254, lr=0.0001]Steps:  16%|█▌        | 78/500 [12:34<22:23,  3.18s/it, loss=0.172, lr=0.0001]Steps:  16%|█▌        | 79/500 [12:37<22:19,  3.18s/it, loss=0.172, lr=0.0001]Steps:  16%|█▌        | 79/500 [12:37<22:19,  3.18s/it, loss=0.482, lr=0.0001]Steps:  16%|█▌        | 80/500 [12:40<22:12,  3.17s/it, loss=0.482, lr=0.0001]Steps:  16%|█▌        | 80/500 [12:40<22:12,  3.17s/it, loss=0.358, lr=0.0001]Steps:  16%|█▌        | 81/500 [12:43<22:11,  3.18s/it, loss=0.358, lr=0.0001]Steps:  16%|█▌        | 81/500 [12:43<22:11,  3.18s/it, loss=0.161, lr=0.0001]Steps:  16%|█▋        | 82/500 [12:46<22:06,  3.17s/it, loss=0.161, lr=0.0001]Steps:  16%|█▋        | 82/500 [12:47<22:06,  3.17s/it, loss=0.196, lr=0.0001]Steps:  17%|█▋        | 83/500 [12:50<22:03,  3.17s/it, loss=0.196, lr=0.0001]Steps:  17%|█▋        | 83/500 [12:50<22:03,  3.17s/it, loss=0.179, lr=0.0001]Steps:  17%|█▋        | 84/500 [12:53<21:59,  3.17s/it, loss=0.179, lr=0.0001]Steps:  17%|█▋        | 84/500 [12:53<21:59,  3.17s/it, loss=0.497, lr=0.0001]Steps:  17%|█▋        | 85/500 [12:56<21:55,  3.17s/it, loss=0.497, lr=0.0001]Steps:  17%|█▋        | 85/500 [12:56<21:55,  3.17s/it, loss=0.0967, lr=0.0001]Steps:  17%|█▋        | 86/500 [12:59<21:53,  3.17s/it, loss=0.0967, lr=0.0001]Steps:  17%|█▋        | 86/500 [12:59<21:53,  3.17s/it, loss=0.201, lr=0.0001] Steps:  17%|█▋        | 87/500 [13:02<21:49,  3.17s/it, loss=0.201, lr=0.0001]Steps:  17%|█▋        | 87/500 [13:02<21:49,  3.17s/it, loss=0.361, lr=0.0001]Steps:  18%|█▊        | 88/500 [13:05<21:46,  3.17s/it, loss=0.361, lr=0.0001]Steps:  18%|█▊        | 88/500 [13:06<21:46,  3.17s/it, loss=0.466, lr=0.0001]Steps:  18%|█▊        | 89/500 [13:09<21:44,  3.17s/it, loss=0.466, lr=0.0001]Steps:  18%|█▊        | 89/500 [13:09<21:44,  3.17s/it, loss=0.453, lr=0.0001]Steps:  18%|█▊        | 90/500 [13:12<21:37,  3.17s/it, loss=0.453, lr=0.0001]Steps:  18%|█▊        | 90/500 [13:12<21:37,  3.17s/it, loss=0.44, lr=0.0001] Steps:  18%|█▊        | 91/500 [13:15<22:01,  3.23s/it, loss=0.44, lr=0.0001]Steps:  18%|█▊        | 91/500 [13:15<22:01,  3.23s/it, loss=0.263, lr=0.0001]Steps:  18%|█▊        | 92/500 [13:18<21:51,  3.21s/it, loss=0.263, lr=0.0001]Steps:  18%|█▊        | 92/500 [13:18<21:51,  3.21s/it, loss=0.177, lr=0.0001]Steps:  19%|█▊        | 93/500 [13:22<21:42,  3.20s/it, loss=0.177, lr=0.0001]Steps:  19%|█▊        | 93/500 [13:22<21:42,  3.20s/it, loss=0.464, lr=0.0001]Steps:  19%|█▉        | 94/500 [13:25<21:35,  3.19s/it, loss=0.464, lr=0.0001]Steps:  19%|█▉        | 94/500 [13:25<21:35,  3.19s/it, loss=0.229, lr=0.0001]Steps:  19%|█▉        | 95/500 [13:28<21:29,  3.18s/it, loss=0.229, lr=0.0001]Steps:  19%|█▉        | 95/500 [13:28<21:29,  3.18s/it, loss=0.165, lr=0.0001]Steps:  19%|█▉        | 96/500 [13:31<21:24,  3.18s/it, loss=0.165, lr=0.0001]Steps:  19%|█▉        | 96/500 [13:31<21:24,  3.18s/it, loss=0.11, lr=0.0001] Steps:  19%|█▉        | 97/500 [13:34<21:19,  3.18s/it, loss=0.11, lr=0.0001]Steps:  19%|█▉        | 97/500 [13:34<21:19,  3.18s/it, loss=0.279, lr=0.0001]Steps:  20%|█▉        | 98/500 [13:37<21:16,  3.18s/it, loss=0.279, lr=0.0001]Steps:  20%|█▉        | 98/500 [13:37<21:16,  3.18s/it, loss=3.87, lr=0.0001] Steps:  20%|█▉        | 99/500 [13:41<21:13,  3.18s/it, loss=3.87, lr=0.0001]Steps:  20%|█▉        | 99/500 [13:41<21:13,  3.18s/it, loss=0.276, lr=0.0001]Steps:  20%|██        | 100/500 [13:44<21:06,  3.17s/it, loss=0.276, lr=0.0001]05/07/2025 00:43:14 - INFO - __main__ - Running validation at step 100
05/07/2025 00:43:14 - INFO - __main__ - Running validation... 
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][AInstantiating AutoencoderKL model under default dtype torch.bfloat16.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at /root/autodl-tmp/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/vae.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
Loaded vae as AutoencoderKL from `vae` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.

Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 17.45it/s][A{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A[ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 72.32it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 31.02it/s]
05/07/2025 00:43:16 - INFO - __main__ - Loaded 6 prompts from validation_prompts.json
05/07/2025 00:43:58 - INFO - __main__ - Generated image 1/2 for prompt 1/6
05/07/2025 00:44:39 - INFO - __main__ - Generated image 2/2 for prompt 1/6
05/07/2025 00:45:21 - INFO - __main__ - Generated image 1/2 for prompt 2/6
05/07/2025 00:46:02 - INFO - __main__ - Generated image 2/2 for prompt 2/6
05/07/2025 00:46:43 - INFO - __main__ - Generated image 1/2 for prompt 3/6
05/07/2025 00:47:25 - INFO - __main__ - Generated image 2/2 for prompt 3/6
05/07/2025 00:48:06 - INFO - __main__ - Generated image 1/2 for prompt 4/6
05/07/2025 00:48:47 - INFO - __main__ - Generated image 2/2 for prompt 4/6
05/07/2025 00:49:29 - INFO - __main__ - Generated image 1/2 for prompt 5/6
05/07/2025 00:50:10 - INFO - __main__ - Generated image 2/2 for prompt 5/6
05/07/2025 00:50:51 - INFO - __main__ - Generated image 1/2 for prompt 6/6
05/07/2025 00:51:33 - INFO - __main__ - Generated image 2/2 for prompt 6/6
Steps:  20%|██        | 100/500 [22:03<21:06,  3.17s/it, loss=0.22, lr=0.0001] Steps:  20%|██        | 101/500 [22:06<16:57:25, 153.00s/it, loss=0.22, lr=0.0001]Steps:  20%|██        | 101/500 [22:06<16:57:25, 153.00s/it, loss=0.173, lr=0.0001]Steps:  20%|██        | 102/500 [22:09<11:56:41, 108.05s/it, loss=0.173, lr=0.0001]Steps:  20%|██        | 102/500 [22:10<11:56:41, 108.05s/it, loss=0.475, lr=0.0001]Steps:  21%|██        | 103/500 [22:13<8:26:43, 76.58s/it, loss=0.475, lr=0.0001]  Steps:  21%|██        | 103/500 [22:13<8:26:43, 76.58s/it, loss=0.0914, lr=0.0001]Steps:  21%|██        | 104/500 [22:16<6:00:05, 54.56s/it, loss=0.0914, lr=0.0001]Steps:  21%|██        | 104/500 [22:16<6:00:05, 54.56s/it, loss=0.215, lr=0.0001] Steps:  21%|██        | 105/500 [22:19<4:17:41, 39.14s/it, loss=0.215, lr=0.0001]Steps:  21%|██        | 105/500 [22:19<4:17:41, 39.14s/it, loss=0.105, lr=0.0001]Steps:  21%|██        | 106/500 [22:22<3:06:10, 28.35s/it, loss=0.105, lr=0.0001]Steps:  21%|██        | 106/500 [22:22<3:06:10, 28.35s/it, loss=0.233, lr=0.0001]Steps:  21%|██▏       | 107/500 [22:25<2:16:12, 20.80s/it, loss=0.233, lr=0.0001]Steps:  21%|██▏       | 107/500 [22:25<2:16:12, 20.80s/it, loss=0.458, lr=0.0001]Steps:  22%|██▏       | 108/500 [22:28<1:41:19, 15.51s/it, loss=0.458, lr=0.0001]Steps:  22%|██▏       | 108/500 [22:29<1:41:19, 15.51s/it, loss=0.159, lr=0.0001]Steps:  22%|██▏       | 109/500 [22:32<1:16:56, 11.81s/it, loss=0.159, lr=0.0001]Steps:  22%|██▏       | 109/500 [22:32<1:16:56, 11.81s/it, loss=0.377, lr=0.0001]Steps:  22%|██▏       | 110/500 [22:35<59:52,  9.21s/it, loss=0.377, lr=0.0001]  Steps:  22%|██▏       | 110/500 [22:35<59:52,  9.21s/it, loss=0.0954, lr=0.0001]Steps:  22%|██▏       | 111/500 [22:38<48:02,  7.41s/it, loss=0.0954, lr=0.0001]Steps:  22%|██▏       | 111/500 [22:38<48:02,  7.41s/it, loss=0.507, lr=0.0001] Steps:  22%|██▏       | 112/500 [22:41<39:42,  6.14s/it, loss=0.507, lr=0.0001]Steps:  22%|██▏       | 112/500 [22:41<39:42,  6.14s/it, loss=0.243, lr=0.0001]Steps:  23%|██▎       | 113/500 [22:44<33:50,  5.25s/it, loss=0.243, lr=0.0001]Steps:  23%|██▎       | 113/500 [22:44<33:50,  5.25s/it, loss=0.42, lr=0.0001] Steps:  23%|██▎       | 114/500 [22:48<29:45,  4.62s/it, loss=0.42, lr=0.0001]Steps:  23%|██▎       | 114/500 [22:48<29:45,  4.62s/it, loss=0.184, lr=0.0001]Steps:  23%|██▎       | 115/500 [22:51<26:52,  4.19s/it, loss=0.184, lr=0.0001]Steps:  23%|██▎       | 115/500 [22:51<26:52,  4.19s/it, loss=0.156, lr=0.0001]Steps:  23%|██▎       | 116/500 [22:54<24:50,  3.88s/it, loss=0.156, lr=0.0001]Steps:  23%|██▎       | 116/500 [22:54<24:50,  3.88s/it, loss=0.245, lr=0.0001]Steps:  23%|██▎       | 117/500 [22:57<23:25,  3.67s/it, loss=0.245, lr=0.0001]Steps:  23%|██▎       | 117/500 [22:57<23:25,  3.67s/it, loss=0.325, lr=0.0001]Steps:  24%|██▎       | 118/500 [23:00<22:23,  3.52s/it, loss=0.325, lr=0.0001]Steps:  24%|██▎       | 118/500 [23:00<22:23,  3.52s/it, loss=0.46, lr=0.0001] Steps:  24%|██▍       | 119/500 [23:03<21:48,  3.43s/it, loss=0.46, lr=0.0001]Steps:  24%|██▍       | 119/500 [23:04<21:48,  3.43s/it, loss=0.144, lr=0.0001]Steps:  24%|██▍       | 120/500 [23:07<21:12,  3.35s/it, loss=0.144, lr=0.0001]Steps:  24%|██▍       | 120/500 [23:07<21:12,  3.35s/it, loss=0.0939, lr=0.0001]Steps:  24%|██▍       | 121/500 [23:10<20:52,  3.30s/it, loss=0.0939, lr=0.0001]Steps:  24%|██▍       | 121/500 [23:10<20:52,  3.30s/it, loss=0.298, lr=0.0001] Steps:  24%|██▍       | 122/500 [23:13<20:33,  3.26s/it, loss=0.298, lr=0.0001]Steps:  24%|██▍       | 122/500 [23:13<20:33,  3.26s/it, loss=0.397, lr=0.0001]Steps:  25%|██▍       | 123/500 [23:16<20:19,  3.24s/it, loss=0.397, lr=0.0001]Steps:  25%|██▍       | 123/500 [23:16<20:19,  3.24s/it, loss=0.0867, lr=0.0001]Steps:  25%|██▍       | 124/500 [23:19<20:10,  3.22s/it, loss=0.0867, lr=0.0001]Steps:  25%|██▍       | 124/500 [23:19<20:10,  3.22s/it, loss=0.238, lr=0.0001] Steps:  25%|██▌       | 125/500 [23:22<20:01,  3.20s/it, loss=0.238, lr=0.0001]Steps:  25%|██▌       | 125/500 [23:23<20:01,  3.20s/it, loss=0.275, lr=0.0001]Steps:  25%|██▌       | 126/500 [23:26<19:54,  3.19s/it, loss=0.275, lr=0.0001]Steps:  25%|██▌       | 126/500 [23:26<19:54,  3.19s/it, loss=0.0906, lr=0.0001]Steps:  25%|██▌       | 127/500 [23:29<19:49,  3.19s/it, loss=0.0906, lr=0.0001]Steps:  25%|██▌       | 127/500 [23:29<19:49,  3.19s/it, loss=0.49, lr=0.0001]  Steps:  26%|██▌       | 128/500 [23:32<19:43,  3.18s/it, loss=0.49, lr=0.0001]Steps:  26%|██▌       | 128/500 [23:32<19:43,  3.18s/it, loss=0.21, lr=0.0001]Steps:  26%|██▌       | 129/500 [23:35<19:39,  3.18s/it, loss=0.21, lr=0.0001]Steps:  26%|██▌       | 129/500 [23:35<19:39,  3.18s/it, loss=0.0848, lr=0.0001]Steps:  26%|██▌       | 130/500 [23:38<19:32,  3.17s/it, loss=0.0848, lr=0.0001]Steps:  26%|██▌       | 130/500 [23:38<19:32,  3.17s/it, loss=0.296, lr=0.0001] Steps:  26%|██▌       | 131/500 [23:41<19:32,  3.18s/it, loss=0.296, lr=0.0001]Steps:  26%|██▌       | 131/500 [23:42<19:32,  3.18s/it, loss=0.475, lr=0.0001]Steps:  26%|██▋       | 132/500 [23:45<19:29,  3.18s/it, loss=0.475, lr=0.0001]Steps:  26%|██▋       | 132/500 [23:45<19:29,  3.18s/it, loss=0.399, lr=0.0001]Steps:  27%|██▋       | 133/500 [23:48<19:24,  3.17s/it, loss=0.399, lr=0.0001]Steps:  27%|██▋       | 133/500 [23:48<19:24,  3.17s/it, loss=0.219, lr=0.0001]Steps:  27%|██▋       | 134/500 [23:51<19:21,  3.17s/it, loss=0.219, lr=0.0001]Steps:  27%|██▋       | 134/500 [23:51<19:21,  3.17s/it, loss=0.2, lr=0.0001]  Steps:  27%|██▋       | 135/500 [23:54<19:18,  3.17s/it, loss=0.2, lr=0.0001]Steps:  27%|██▋       | 135/500 [23:54<19:18,  3.17s/it, loss=0.47, lr=0.0001]Steps:  27%|██▋       | 136/500 [23:57<19:14,  3.17s/it, loss=0.47, lr=0.0001]Steps:  27%|██▋       | 136/500 [23:57<19:14,  3.17s/it, loss=0.346, lr=0.0001]Steps:  27%|██▋       | 137/500 [24:01<19:10,  3.17s/it, loss=0.346, lr=0.0001]Steps:  27%|██▋       | 137/500 [24:01<19:10,  3.17s/it, loss=0.125, lr=0.0001]Steps:  28%|██▊       | 138/500 [24:04<19:07,  3.17s/it, loss=0.125, lr=0.0001]Steps:  28%|██▊       | 138/500 [24:04<19:07,  3.17s/it, loss=0.165, lr=0.0001]Steps:  28%|██▊       | 139/500 [24:07<19:04,  3.17s/it, loss=0.165, lr=0.0001]Steps:  28%|██▊       | 139/500 [24:07<19:04,  3.17s/it, loss=0.964, lr=0.0001]Steps:  28%|██▊       | 140/500 [24:10<18:58,  3.16s/it, loss=0.964, lr=0.0001]Steps:  28%|██▊       | 140/500 [24:10<18:58,  3.16s/it, loss=0.225, lr=0.0001]Steps:  28%|██▊       | 141/500 [24:13<19:00,  3.18s/it, loss=0.225, lr=0.0001]Steps:  28%|██▊       | 141/500 [24:13<19:00,  3.18s/it, loss=0.474, lr=0.0001]Steps:  28%|██▊       | 142/500 [24:16<18:56,  3.18s/it, loss=0.474, lr=0.0001]Steps:  28%|██▊       | 142/500 [24:16<18:56,  3.18s/it, loss=0.224, lr=0.0001]Steps:  29%|██▊       | 143/500 [24:20<18:52,  3.17s/it, loss=0.224, lr=0.0001]Steps:  29%|██▊       | 143/500 [24:20<18:52,  3.17s/it, loss=0.132, lr=0.0001]Steps:  29%|██▉       | 144/500 [24:23<18:48,  3.17s/it, loss=0.132, lr=0.0001]Steps:  29%|██▉       | 144/500 [24:23<18:48,  3.17s/it, loss=0.102, lr=0.0001]Steps:  29%|██▉       | 145/500 [24:26<18:45,  3.17s/it, loss=0.102, lr=0.0001]Steps:  29%|██▉       | 145/500 [24:26<18:45,  3.17s/it, loss=0.112, lr=0.0001]Steps:  29%|██▉       | 146/500 [24:29<18:42,  3.17s/it, loss=0.112, lr=0.0001]Steps:  29%|██▉       | 146/500 [24:29<18:42,  3.17s/it, loss=0.144, lr=0.0001]Steps:  29%|██▉       | 147/500 [24:32<18:47,  3.19s/it, loss=0.144, lr=0.0001]Steps:  29%|██▉       | 147/500 [24:32<18:47,  3.19s/it, loss=0.499, lr=0.0001]Steps:  30%|██▉       | 148/500 [24:35<18:42,  3.19s/it, loss=0.499, lr=0.0001]Steps:  30%|██▉       | 148/500 [24:36<18:42,  3.19s/it, loss=0.0843, lr=0.0001]Steps:  30%|██▉       | 149/500 [24:39<18:37,  3.18s/it, loss=0.0843, lr=0.0001]Steps:  30%|██▉       | 149/500 [24:39<18:37,  3.18s/it, loss=0.217, lr=0.0001] Steps:  30%|███       | 150/500 [24:42<18:30,  3.17s/it, loss=0.217, lr=0.0001]05/07/2025 00:54:12 - INFO - __main__ - Running validation at step 150
05/07/2025 00:54:12 - INFO - __main__ - Running validation... 
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][AInstantiating AutoencoderKL model under default dtype torch.bfloat16.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at /root/autodl-tmp/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/vae.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
Loaded vae as AutoencoderKL from `vae` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.

Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 17.51it/s][A{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A[ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 67.47it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 30.79it/s]
05/07/2025 00:54:14 - INFO - __main__ - Loaded 6 prompts from validation_prompts.json
05/07/2025 00:54:56 - INFO - __main__ - Generated image 1/2 for prompt 1/6
05/07/2025 00:55:37 - INFO - __main__ - Generated image 2/2 for prompt 1/6
05/07/2025 00:56:18 - INFO - __main__ - Generated image 1/2 for prompt 2/6
05/07/2025 00:57:00 - INFO - __main__ - Generated image 2/2 for prompt 2/6
05/07/2025 00:57:41 - INFO - __main__ - Generated image 1/2 for prompt 3/6
05/07/2025 00:58:22 - INFO - __main__ - Generated image 2/2 for prompt 3/6
05/07/2025 00:59:04 - INFO - __main__ - Generated image 1/2 for prompt 4/6
05/07/2025 00:59:45 - INFO - __main__ - Generated image 2/2 for prompt 4/6
05/07/2025 01:00:27 - INFO - __main__ - Generated image 1/2 for prompt 5/6
05/07/2025 01:01:08 - INFO - __main__ - Generated image 2/2 for prompt 5/6
05/07/2025 01:01:49 - INFO - __main__ - Generated image 1/2 for prompt 6/6
05/07/2025 01:02:31 - INFO - __main__ - Generated image 2/2 for prompt 6/6
Steps:  30%|███       | 150/500 [33:01<18:30,  3.17s/it, loss=0.319, lr=0.0001]Steps:  30%|███       | 151/500 [33:04<14:49:24, 152.91s/it, loss=0.319, lr=0.0001]Steps:  30%|███       | 151/500 [33:04<14:49:24, 152.91s/it, loss=0.0876, lr=0.0001]Steps:  30%|███       | 152/500 [33:07<10:26:19, 107.99s/it, loss=0.0876, lr=0.0001]Steps:  30%|███       | 152/500 [33:07<10:26:19, 107.99s/it, loss=0.16, lr=0.0001]  Steps:  31%|███       | 153/500 [33:10<7:22:42, 76.55s/it, loss=0.16, lr=0.0001]  Steps:  31%|███       | 153/500 [33:11<7:22:42, 76.55s/it, loss=0.316, lr=0.0001]Steps:  31%|███       | 154/500 [33:14<5:14:30, 54.54s/it, loss=0.316, lr=0.0001]Steps:  31%|███       | 154/500 [33:14<5:14:30, 54.54s/it, loss=0.259, lr=0.0001]Steps:  31%|███       | 155/500 [33:17<3:44:58, 39.13s/it, loss=0.259, lr=0.0001]Steps:  31%|███       | 155/500 [33:17<3:44:58, 39.13s/it, loss=0.488, lr=0.0001]Steps:  31%|███       | 156/500 [33:20<2:42:29, 28.34s/it, loss=0.488, lr=0.0001]Steps:  31%|███       | 156/500 [33:20<2:42:29, 28.34s/it, loss=0.121, lr=0.0001]Steps:  31%|███▏      | 157/500 [33:23<1:58:50, 20.79s/it, loss=0.121, lr=0.0001]Steps:  31%|███▏      | 157/500 [33:23<1:58:50, 20.79s/it, loss=0.347, lr=0.0001]Steps:  32%|███▏      | 158/500 [33:26<1:28:21, 15.50s/it, loss=0.347, lr=0.0001]Steps:  32%|███▏      | 158/500 [33:26<1:28:21, 15.50s/it, loss=0.167, lr=0.0001]Steps:  32%|███▏      | 159/500 [33:29<1:07:05, 11.80s/it, loss=0.167, lr=0.0001]Steps:  32%|███▏      | 159/500 [33:30<1:07:05, 11.80s/it, loss=1.03, lr=0.0001] Steps:  32%|███▏      | 160/500 [33:33<52:10,  9.21s/it, loss=1.03, lr=0.0001]  Steps:  32%|███▏      | 160/500 [33:33<52:10,  9.21s/it, loss=0.306, lr=0.0001]Steps:  32%|███▏      | 161/500 [33:36<41:51,  7.41s/it, loss=0.306, lr=0.0001]Steps:  32%|███▏      | 161/500 [33:36<41:51,  7.41s/it, loss=0.103, lr=0.0001]Steps:  32%|███▏      | 162/500 [33:39<34:34,  6.14s/it, loss=0.103, lr=0.0001]Steps:  32%|███▏      | 162/500 [33:39<34:34,  6.14s/it, loss=0.451, lr=0.0001]Steps:  33%|███▎      | 163/500 [33:42<29:28,  5.25s/it, loss=0.451, lr=0.0001]Steps:  33%|███▎      | 163/500 [33:42<29:28,  5.25s/it, loss=0.0796, lr=0.0001]Steps:  33%|███▎      | 164/500 [33:45<25:54,  4.63s/it, loss=0.0796, lr=0.0001]Steps:  33%|███▎      | 164/500 [33:45<25:54,  4.63s/it, loss=0.253, lr=0.0001] Steps:  33%|███▎      | 165/500 [33:49<23:23,  4.19s/it, loss=0.253, lr=0.0001]Steps:  33%|███▎      | 165/500 [33:49<23:23,  4.19s/it, loss=0.298, lr=0.0001]Steps:  33%|███▎      | 166/500 [33:52<21:37,  3.88s/it, loss=0.298, lr=0.0001]Steps:  33%|███▎      | 166/500 [33:52<21:37,  3.88s/it, loss=0.12, lr=0.0001] Steps:  33%|███▎      | 167/500 [33:55<20:24,  3.68s/it, loss=0.12, lr=0.0001]Steps:  33%|███▎      | 167/500 [33:55<20:24,  3.68s/it, loss=0.417, lr=0.0001]Steps:  34%|███▎      | 168/500 [33:58<19:29,  3.52s/it, loss=0.417, lr=0.0001]Steps:  34%|███▎      | 168/500 [33:58<19:29,  3.52s/it, loss=0.233, lr=0.0001]Steps:  34%|███▍      | 169/500 [34:01<18:57,  3.44s/it, loss=0.233, lr=0.0001]Steps:  34%|███▍      | 169/500 [34:01<18:57,  3.44s/it, loss=0.176, lr=0.0001]Steps:  34%|███▍      | 170/500 [34:04<18:25,  3.35s/it, loss=0.176, lr=0.0001]Steps:  34%|███▍      | 170/500 [34:05<18:25,  3.35s/it, loss=0.189, lr=0.0001]Steps:  34%|███▍      | 171/500 [34:08<18:08,  3.31s/it, loss=0.189, lr=0.0001]Steps:  34%|███▍      | 171/500 [34:08<18:08,  3.31s/it, loss=0.444, lr=0.0001]Steps:  34%|███▍      | 172/500 [34:11<17:51,  3.27s/it, loss=0.444, lr=0.0001]Steps:  34%|███▍      | 172/500 [34:11<17:51,  3.27s/it, loss=0.162, lr=0.0001]Steps:  35%|███▍      | 173/500 [34:14<17:38,  3.24s/it, loss=0.162, lr=0.0001]Steps:  35%|███▍      | 173/500 [34:14<17:38,  3.24s/it, loss=0.472, lr=0.0001]Steps:  35%|███▍      | 174/500 [34:17<17:28,  3.22s/it, loss=0.472, lr=0.0001]Steps:  35%|███▍      | 174/500 [34:17<17:28,  3.22s/it, loss=0.177, lr=0.0001]Steps:  35%|███▌      | 175/500 [34:20<17:20,  3.20s/it, loss=0.177, lr=0.0001]Steps:  35%|███▌      | 175/500 [34:20<17:20,  3.20s/it, loss=0.158, lr=0.0001]Steps:  35%|███▌      | 176/500 [34:24<17:15,  3.20s/it, loss=0.158, lr=0.0001]Steps:  35%|███▌      | 176/500 [34:24<17:15,  3.20s/it, loss=0.329, lr=0.0001]Steps:  35%|███▌      | 177/500 [34:27<17:10,  3.19s/it, loss=0.329, lr=0.0001]Steps:  35%|███▌      | 177/500 [34:27<17:10,  3.19s/it, loss=0.252, lr=0.0001]Steps:  36%|███▌      | 178/500 [34:30<17:04,  3.18s/it, loss=0.252, lr=0.0001]Steps:  36%|███▌      | 178/500 [34:30<17:04,  3.18s/it, loss=0.223, lr=0.0001]Steps:  36%|███▌      | 179/500 [34:33<17:00,  3.18s/it, loss=0.223, lr=0.0001]Steps:  36%|███▌      | 179/500 [34:33<17:00,  3.18s/it, loss=0.0979, lr=0.0001]Steps:  36%|███▌      | 180/500 [34:36<16:54,  3.17s/it, loss=0.0979, lr=0.0001]Steps:  36%|███▌      | 180/500 [34:36<16:54,  3.17s/it, loss=0.221, lr=0.0001] Steps:  36%|███▌      | 181/500 [34:39<16:54,  3.18s/it, loss=0.221, lr=0.0001]Steps:  36%|███▌      | 181/500 [34:39<16:54,  3.18s/it, loss=0.255, lr=0.0001]Steps:  36%|███▋      | 182/500 [34:43<16:51,  3.18s/it, loss=0.255, lr=0.0001]Steps:  36%|███▋      | 182/500 [34:43<16:51,  3.18s/it, loss=0.185, lr=0.0001]Steps:  37%|███▋      | 183/500 [34:46<16:47,  3.18s/it, loss=0.185, lr=0.0001]Steps:  37%|███▋      | 183/500 [34:46<16:47,  3.18s/it, loss=0.296, lr=0.0001]Steps:  37%|███▋      | 184/500 [34:49<16:43,  3.18s/it, loss=0.296, lr=0.0001]Steps:  37%|███▋      | 184/500 [34:49<16:43,  3.18s/it, loss=0.238, lr=0.0001]Steps:  37%|███▋      | 185/500 [34:52<16:39,  3.17s/it, loss=0.238, lr=0.0001]Steps:  37%|███▋      | 185/500 [34:52<16:39,  3.17s/it, loss=0.173, lr=0.0001]Steps:  37%|███▋      | 186/500 [34:55<16:36,  3.17s/it, loss=0.173, lr=0.0001]Steps:  37%|███▋      | 186/500 [34:55<16:36,  3.17s/it, loss=0.115, lr=0.0001]Steps:  37%|███▋      | 187/500 [34:58<16:32,  3.17s/it, loss=0.115, lr=0.0001]Steps:  37%|███▋      | 187/500 [34:59<16:32,  3.17s/it, loss=0.234, lr=0.0001]Steps:  38%|███▊      | 188/500 [35:02<16:29,  3.17s/it, loss=0.234, lr=0.0001]Steps:  38%|███▊      | 188/500 [35:02<16:29,  3.17s/it, loss=0.0927, lr=0.0001]Steps:  38%|███▊      | 189/500 [35:05<16:26,  3.17s/it, loss=0.0927, lr=0.0001]Steps:  38%|███▊      | 189/500 [35:05<16:26,  3.17s/it, loss=0.476, lr=0.0001] Steps:  38%|███▊      | 190/500 [35:08<16:20,  3.16s/it, loss=0.476, lr=0.0001]Steps:  38%|███▊      | 190/500 [35:08<16:20,  3.16s/it, loss=0.179, lr=0.0001]Steps:  38%|███▊      | 191/500 [35:11<16:20,  3.17s/it, loss=0.179, lr=0.0001]Steps:  38%|███▊      | 191/500 [35:11<16:20,  3.17s/it, loss=0.0885, lr=0.0001]Steps:  38%|███▊      | 192/500 [35:14<16:16,  3.17s/it, loss=0.0885, lr=0.0001]Steps:  38%|███▊      | 192/500 [35:14<16:16,  3.17s/it, loss=0.0835, lr=0.0001]Steps:  39%|███▊      | 193/500 [35:17<16:13,  3.17s/it, loss=0.0835, lr=0.0001]Steps:  39%|███▊      | 193/500 [35:18<16:13,  3.17s/it, loss=0.171, lr=0.0001] Steps:  39%|███▉      | 194/500 [35:21<16:11,  3.17s/it, loss=0.171, lr=0.0001]Steps:  39%|███▉      | 194/500 [35:21<16:11,  3.17s/it, loss=0.396, lr=0.0001]Steps:  39%|███▉      | 195/500 [35:24<16:07,  3.17s/it, loss=0.396, lr=0.0001]Steps:  39%|███▉      | 195/500 [35:24<16:07,  3.17s/it, loss=0.362, lr=0.0001]Steps:  39%|███▉      | 196/500 [35:27<16:03,  3.17s/it, loss=0.362, lr=0.0001]Steps:  39%|███▉      | 196/500 [35:27<16:03,  3.17s/it, loss=0.414, lr=0.0001]Steps:  39%|███▉      | 197/500 [35:30<16:07,  3.19s/it, loss=0.414, lr=0.0001]Steps:  39%|███▉      | 197/500 [35:30<16:07,  3.19s/it, loss=0.17, lr=0.0001] Steps:  40%|███▉      | 198/500 [35:33<16:02,  3.19s/it, loss=0.17, lr=0.0001]Steps:  40%|███▉      | 198/500 [35:33<16:02,  3.19s/it, loss=0.124, lr=0.0001]Steps:  40%|███▉      | 199/500 [35:37<15:58,  3.18s/it, loss=0.124, lr=0.0001]Steps:  40%|███▉      | 199/500 [35:37<15:58,  3.18s/it, loss=0.478, lr=0.0001]Steps:  40%|████      | 200/500 [35:40<15:52,  3.17s/it, loss=0.478, lr=0.0001]05/07/2025 01:05:10 - INFO - __main__ - Running validation at step 200
05/07/2025 01:05:10 - INFO - __main__ - Running validation... 
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][AInstantiating AutoencoderKL model under default dtype torch.bfloat16.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at /root/autodl-tmp/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/vae.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
Loaded vae as AutoencoderKL from `vae` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.

Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 17.76it/s][A{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A[ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 69.44it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 31.29it/s]
05/07/2025 01:05:12 - INFO - __main__ - Loaded 6 prompts from validation_prompts.json
05/07/2025 01:05:54 - INFO - __main__ - Generated image 1/2 for prompt 1/6
05/07/2025 01:06:35 - INFO - __main__ - Generated image 2/2 for prompt 1/6
05/07/2025 01:07:16 - INFO - __main__ - Generated image 1/2 for prompt 2/6
05/07/2025 01:07:58 - INFO - __main__ - Generated image 2/2 for prompt 2/6
05/07/2025 01:08:39 - INFO - __main__ - Generated image 1/2 for prompt 3/6
05/07/2025 01:09:20 - INFO - __main__ - Generated image 2/2 for prompt 3/6
05/07/2025 01:10:02 - INFO - __main__ - Generated image 1/2 for prompt 4/6
05/07/2025 01:10:43 - INFO - __main__ - Generated image 2/2 for prompt 4/6
05/07/2025 01:11:25 - INFO - __main__ - Generated image 1/2 for prompt 5/6
05/07/2025 01:12:06 - INFO - __main__ - Generated image 2/2 for prompt 5/6
05/07/2025 01:12:47 - INFO - __main__ - Generated image 1/2 for prompt 6/6
05/07/2025 01:13:29 - INFO - __main__ - Generated image 2/2 for prompt 6/6
Steps:  40%|████      | 200/500 [43:59<15:52,  3.17s/it, loss=0.504, lr=0.0001]Steps:  40%|████      | 201/500 [44:02<12:42:16, 152.96s/it, loss=0.504, lr=0.0001]Steps:  40%|████      | 201/500 [44:02<12:42:16, 152.96s/it, loss=0.407, lr=0.0001]Steps:  40%|████      | 202/500 [44:05<8:56:31, 108.03s/it, loss=0.407, lr=0.0001] Steps:  40%|████      | 202/500 [44:05<8:56:31, 108.03s/it, loss=0.28, lr=0.0001] Steps:  41%|████      | 203/500 [44:09<6:19:00, 76.57s/it, loss=0.28, lr=0.0001] Steps:  41%|████      | 203/500 [44:09<6:19:00, 76.57s/it, loss=0.5, lr=0.0001] Steps:  41%|████      | 204/500 [44:12<4:29:06, 54.55s/it, loss=0.5, lr=0.0001]Steps:  41%|████      | 204/500 [44:12<4:29:06, 54.55s/it, loss=0.206, lr=0.0001]Steps:  41%|████      | 205/500 [44:15<3:12:24, 39.13s/it, loss=0.206, lr=0.0001]Steps:  41%|████      | 205/500 [44:15<3:12:24, 39.13s/it, loss=0.115, lr=0.0001]Steps:  41%|████      | 206/500 [44:18<2:18:53, 28.35s/it, loss=0.115, lr=0.0001]Steps:  41%|████      | 206/500 [44:18<2:18:53, 28.35s/it, loss=0.265, lr=0.0001]Steps:  41%|████▏     | 207/500 [44:21<1:41:32, 20.79s/it, loss=0.265, lr=0.0001]Steps:  41%|████▏     | 207/500 [44:21<1:41:32, 20.79s/it, loss=0.221, lr=0.0001]Steps:  42%|████▏     | 208/500 [44:24<1:15:27, 15.50s/it, loss=0.221, lr=0.0001]Steps:  42%|████▏     | 208/500 [44:24<1:15:27, 15.50s/it, loss=0.122, lr=0.0001]Steps:  42%|████▏     | 209/500 [44:28<57:15, 11.81s/it, loss=0.122, lr=0.0001]  Steps:  42%|████▏     | 209/500 [44:28<57:15, 11.81s/it, loss=0.183, lr=0.0001]Steps:  42%|████▏     | 210/500 [44:31<44:30,  9.21s/it, loss=0.183, lr=0.0001]Steps:  42%|████▏     | 210/500 [44:31<44:30,  9.21s/it, loss=0.531, lr=0.0001]Steps:  42%|████▏     | 211/500 [44:34<35:39,  7.40s/it, loss=0.531, lr=0.0001]Steps:  42%|████▏     | 211/500 [44:34<35:39,  7.40s/it, loss=0.101, lr=0.0001]Steps:  42%|████▏     | 212/500 [44:37<29:25,  6.13s/it, loss=0.101, lr=0.0001]Steps:  42%|████▏     | 212/500 [44:37<29:25,  6.13s/it, loss=0.195, lr=0.0001]Steps:  43%|████▎     | 213/500 [44:40<25:05,  5.24s/it, loss=0.195, lr=0.0001]Steps:  43%|████▎     | 213/500 [44:40<25:05,  5.24s/it, loss=0.151, lr=0.0001]Steps:  43%|████▎     | 214/500 [44:43<22:01,  4.62s/it, loss=0.151, lr=0.0001]Steps:  43%|████▎     | 214/500 [44:43<22:01,  4.62s/it, loss=0.284, lr=0.0001]Steps:  43%|████▎     | 215/500 [44:47<19:52,  4.18s/it, loss=0.284, lr=0.0001]Steps:  43%|████▎     | 215/500 [44:47<19:52,  4.18s/it, loss=0.139, lr=0.0001]Steps:  43%|████▎     | 216/500 [44:50<18:22,  3.88s/it, loss=0.139, lr=0.0001]Steps:  43%|████▎     | 216/500 [44:50<18:22,  3.88s/it, loss=1.05, lr=0.0001] Steps:  43%|████▎     | 217/500 [44:53<17:18,  3.67s/it, loss=1.05, lr=0.0001]Steps:  43%|████▎     | 217/500 [44:53<17:18,  3.67s/it, loss=0.222, lr=0.0001]Steps:  44%|████▎     | 218/500 [44:56<16:32,  3.52s/it, loss=0.222, lr=0.0001]Steps:  44%|████▎     | 218/500 [44:56<16:32,  3.52s/it, loss=0.389, lr=0.0001]Steps:  44%|████▍     | 219/500 [44:59<16:07,  3.44s/it, loss=0.389, lr=0.0001]Steps:  44%|████▍     | 219/500 [44:59<16:07,  3.44s/it, loss=0.357, lr=0.0001]Steps:  44%|████▍     | 220/500 [45:02<15:39,  3.35s/it, loss=0.357, lr=0.0001]Steps:  44%|████▍     | 220/500 [45:03<15:39,  3.35s/it, loss=0.488, lr=0.0001]Steps:  44%|████▍     | 221/500 [45:06<15:22,  3.31s/it, loss=0.488, lr=0.0001]Steps:  44%|████▍     | 221/500 [45:06<15:22,  3.31s/it, loss=0.182, lr=0.0001]Steps:  44%|████▍     | 222/500 [45:09<15:07,  3.26s/it, loss=0.182, lr=0.0001]Steps:  44%|████▍     | 222/500 [45:09<15:07,  3.26s/it, loss=0.192, lr=0.0001]Steps:  45%|████▍     | 223/500 [45:12<14:57,  3.24s/it, loss=0.192, lr=0.0001]Steps:  45%|████▍     | 223/500 [45:12<14:57,  3.24s/it, loss=0.101, lr=0.0001]Steps:  45%|████▍     | 224/500 [45:15<14:48,  3.22s/it, loss=0.101, lr=0.0001]Steps:  45%|████▍     | 224/500 [45:15<14:48,  3.22s/it, loss=0.28, lr=0.0001] Steps:  45%|████▌     | 225/500 [45:18<14:40,  3.20s/it, loss=0.28, lr=0.0001]Steps:  45%|████▌     | 225/500 [45:18<14:40,  3.20s/it, loss=0.231, lr=0.0001]Steps:  45%|████▌     | 226/500 [45:22<14:35,  3.19s/it, loss=0.231, lr=0.0001]Steps:  45%|████▌     | 226/500 [45:22<14:35,  3.19s/it, loss=0.165, lr=0.0001]Steps:  45%|████▌     | 227/500 [45:25<14:29,  3.19s/it, loss=0.165, lr=0.0001]Steps:  45%|████▌     | 227/500 [45:25<14:29,  3.19s/it, loss=0.29, lr=0.0001] Steps:  46%|████▌     | 228/500 [45:28<14:25,  3.18s/it, loss=0.29, lr=0.0001]Steps:  46%|████▌     | 228/500 [45:28<14:25,  3.18s/it, loss=0.0886, lr=0.0001]Steps:  46%|████▌     | 229/500 [45:31<14:21,  3.18s/it, loss=0.0886, lr=0.0001]Steps:  46%|████▌     | 229/500 [45:31<14:21,  3.18s/it, loss=0.447, lr=0.0001] Steps:  46%|████▌     | 230/500 [45:34<14:16,  3.17s/it, loss=0.447, lr=0.0001]Steps:  46%|████▌     | 230/500 [45:34<14:16,  3.17s/it, loss=0.522, lr=0.0001]Steps:  46%|████▌     | 231/500 [45:37<14:14,  3.18s/it, loss=0.522, lr=0.0001]Steps:  46%|████▌     | 231/500 [45:37<14:14,  3.18s/it, loss=0.37, lr=0.0001] Steps:  46%|████▋     | 232/500 [45:41<14:10,  3.17s/it, loss=0.37, lr=0.0001]Steps:  46%|████▋     | 232/500 [45:41<14:10,  3.17s/it, loss=0.129, lr=0.0001]Steps:  47%|████▋     | 233/500 [45:44<14:07,  3.17s/it, loss=0.129, lr=0.0001]Steps:  47%|████▋     | 233/500 [45:44<14:07,  3.17s/it, loss=0.172, lr=0.0001]Steps:  47%|████▋     | 234/500 [45:47<14:03,  3.17s/it, loss=0.172, lr=0.0001]Steps:  47%|████▋     | 234/500 [45:47<14:03,  3.17s/it, loss=0.475, lr=0.0001]Steps:  47%|████▋     | 235/500 [45:50<14:00,  3.17s/it, loss=0.475, lr=0.0001]Steps:  47%|████▋     | 235/500 [45:50<14:00,  3.17s/it, loss=0.0851, lr=0.0001]Steps:  47%|████▋     | 236/500 [45:53<13:57,  3.17s/it, loss=0.0851, lr=0.0001]Steps:  47%|████▋     | 236/500 [45:53<13:57,  3.17s/it, loss=0.246, lr=0.0001] Steps:  47%|████▋     | 237/500 [45:56<13:53,  3.17s/it, loss=0.246, lr=0.0001]Steps:  47%|████▋     | 237/500 [45:56<13:53,  3.17s/it, loss=0.465, lr=0.0001]Steps:  48%|████▊     | 238/500 [46:00<13:50,  3.17s/it, loss=0.465, lr=0.0001]Steps:  48%|████▊     | 238/500 [46:00<13:50,  3.17s/it, loss=0.0863, lr=0.0001]Steps:  48%|████▊     | 239/500 [46:03<13:48,  3.17s/it, loss=0.0863, lr=0.0001]Steps:  48%|████▊     | 239/500 [46:03<13:48,  3.17s/it, loss=0.323, lr=0.0001] Steps:  48%|████▊     | 240/500 [46:06<13:43,  3.17s/it, loss=0.323, lr=0.0001]Steps:  48%|████▊     | 240/500 [46:06<13:43,  3.17s/it, loss=0.218, lr=0.0001]Steps:  48%|████▊     | 241/500 [46:09<13:42,  3.18s/it, loss=0.218, lr=0.0001]Steps:  48%|████▊     | 241/500 [46:09<13:42,  3.18s/it, loss=0.405, lr=0.0001]Steps:  48%|████▊     | 242/500 [46:12<13:39,  3.18s/it, loss=0.405, lr=0.0001]Steps:  48%|████▊     | 242/500 [46:12<13:39,  3.18s/it, loss=0.412, lr=0.0001]Steps:  49%|████▊     | 243/500 [46:15<13:35,  3.17s/it, loss=0.412, lr=0.0001]Steps:  49%|████▊     | 243/500 [46:16<13:35,  3.17s/it, loss=0.529, lr=0.0001]Steps:  49%|████▉     | 244/500 [46:19<13:31,  3.17s/it, loss=0.529, lr=0.0001]Steps:  49%|████▉     | 244/500 [46:19<13:31,  3.17s/it, loss=0.101, lr=0.0001]Steps:  49%|████▉     | 245/500 [46:22<13:28,  3.17s/it, loss=0.101, lr=0.0001]Steps:  49%|████▉     | 245/500 [46:22<13:28,  3.17s/it, loss=0.178, lr=0.0001]Steps:  49%|████▉     | 246/500 [46:25<13:25,  3.17s/it, loss=0.178, lr=0.0001]Steps:  49%|████▉     | 246/500 [46:25<13:25,  3.17s/it, loss=0.225, lr=0.0001]Steps:  49%|████▉     | 247/500 [46:28<13:27,  3.19s/it, loss=0.225, lr=0.0001]Steps:  49%|████▉     | 247/500 [46:28<13:27,  3.19s/it, loss=0.32, lr=0.0001] Steps:  50%|████▉     | 248/500 [46:31<13:23,  3.19s/it, loss=0.32, lr=0.0001]Steps:  50%|████▉     | 248/500 [46:31<13:23,  3.19s/it, loss=0.376, lr=0.0001]Steps:  50%|████▉     | 249/500 [46:35<13:18,  3.18s/it, loss=0.376, lr=0.0001]Steps:  50%|████▉     | 249/500 [46:35<13:18,  3.18s/it, loss=0.491, lr=0.0001]Steps:  50%|█████     | 250/500 [46:38<13:12,  3.17s/it, loss=0.491, lr=0.0001]05/07/2025 01:16:08 - INFO - __main__ - Running validation at step 250
05/07/2025 01:16:08 - INFO - __main__ - Running validation... 
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][AInstantiating AutoencoderKL model under default dtype torch.bfloat16.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at /root/autodl-tmp/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/vae.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
Loaded vae as AutoencoderKL from `vae` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.

Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 17.70it/s][A{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A[ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 70.30it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 31.22it/s]
05/07/2025 01:16:10 - INFO - __main__ - Loaded 6 prompts from validation_prompts.json
05/07/2025 01:16:52 - INFO - __main__ - Generated image 1/2 for prompt 1/6
05/07/2025 01:17:33 - INFO - __main__ - Generated image 2/2 for prompt 1/6
05/07/2025 01:18:14 - INFO - __main__ - Generated image 1/2 for prompt 2/6
05/07/2025 01:18:56 - INFO - __main__ - Generated image 2/2 for prompt 2/6
05/07/2025 01:19:37 - INFO - __main__ - Generated image 1/2 for prompt 3/6
05/07/2025 01:20:18 - INFO - __main__ - Generated image 2/2 for prompt 3/6
05/07/2025 01:21:00 - INFO - __main__ - Generated image 1/2 for prompt 4/6
05/07/2025 01:21:41 - INFO - __main__ - Generated image 2/2 for prompt 4/6
05/07/2025 01:22:22 - INFO - __main__ - Generated image 1/2 for prompt 5/6
05/07/2025 01:23:04 - INFO - __main__ - Generated image 2/2 for prompt 5/6
05/07/2025 01:23:45 - INFO - __main__ - Generated image 1/2 for prompt 6/6
05/07/2025 01:24:27 - INFO - __main__ - Generated image 2/2 for prompt 6/6
Steps:  50%|█████     | 250/500 [54:57<13:12,  3.17s/it, loss=0.237, lr=0.0001]Steps:  50%|█████     | 251/500 [55:00<10:34:39, 152.93s/it, loss=0.237, lr=0.0001]Steps:  50%|█████     | 251/500 [55:00<10:34:39, 152.93s/it, loss=0.109, lr=0.0001]Steps:  50%|█████     | 252/500 [55:03<7:26:24, 108.00s/it, loss=0.109, lr=0.0001] Steps:  50%|█████     | 252/500 [55:03<7:26:24, 108.00s/it, loss=0.176, lr=0.0001]Steps:  51%|█████     | 253/500 [55:06<5:15:09, 76.55s/it, loss=0.176, lr=0.0001] Steps:  51%|█████     | 253/500 [55:06<5:15:09, 76.55s/it, loss=0.499, lr=0.0001]Steps:  51%|█████     | 254/500 [55:10<3:43:36, 54.54s/it, loss=0.499, lr=0.0001]Steps:  51%|█████     | 254/500 [55:10<3:43:36, 54.54s/it, loss=0.29, lr=0.0001] Steps:  51%|█████     | 255/500 [55:13<2:39:47, 39.13s/it, loss=0.29, lr=0.0001]Steps:  51%|█████     | 255/500 [55:13<2:39:47, 39.13s/it, loss=0.329, lr=0.0001]Steps:  51%|█████     | 256/500 [55:16<1:55:16, 28.35s/it, loss=0.329, lr=0.0001]Steps:  51%|█████     | 256/500 [55:16<1:55:16, 28.35s/it, loss=0.468, lr=0.0001]Steps:  51%|█████▏    | 257/500 [55:19<1:24:12, 20.79s/it, loss=0.468, lr=0.0001]Steps:  51%|█████▏    | 257/500 [55:19<1:24:12, 20.79s/it, loss=0.442, lr=0.0001]Steps:  52%|█████▏    | 258/500 [55:22<1:02:32, 15.51s/it, loss=0.442, lr=0.0001]Steps:  52%|█████▏    | 258/500 [55:22<1:02:32, 15.51s/it, loss=0.478, lr=0.0001]Steps:  52%|█████▏    | 259/500 [55:25<47:25, 11.81s/it, loss=0.478, lr=0.0001]  Steps:  52%|█████▏    | 259/500 [55:26<47:25, 11.81s/it, loss=0.0862, lr=0.0001]Steps:  52%|█████▏    | 260/500 [55:29<36:50,  9.21s/it, loss=0.0862, lr=0.0001]Steps:  52%|█████▏    | 260/500 [55:29<36:50,  9.21s/it, loss=0.0954, lr=0.0001]Steps:  52%|█████▏    | 261/500 [55:32<29:30,  7.41s/it, loss=0.0954, lr=0.0001]Steps:  52%|█████▏    | 261/500 [55:32<29:30,  7.41s/it, loss=0.356, lr=0.0001] Steps:  52%|█████▏    | 262/500 [55:35<24:21,  6.14s/it, loss=0.356, lr=0.0001]Steps:  52%|█████▏    | 262/500 [55:35<24:21,  6.14s/it, loss=0.151, lr=0.0001]Steps:  53%|█████▎    | 263/500 [55:38<20:43,  5.25s/it, loss=0.151, lr=0.0001]Steps:  53%|█████▎    | 263/500 [55:38<20:43,  5.25s/it, loss=0.419, lr=0.0001]Steps:  53%|█████▎    | 264/500 [55:41<18:11,  4.62s/it, loss=0.419, lr=0.0001]Steps:  53%|█████▎    | 264/500 [55:41<18:11,  4.62s/it, loss=0.195, lr=0.0001]Steps:  53%|█████▎    | 265/500 [55:44<16:24,  4.19s/it, loss=0.195, lr=0.0001]Steps:  53%|█████▎    | 265/500 [55:45<16:24,  4.19s/it, loss=0.0827, lr=0.0001]Steps:  53%|█████▎    | 266/500 [55:48<15:08,  3.88s/it, loss=0.0827, lr=0.0001]Steps:  53%|█████▎    | 266/500 [55:48<15:08,  3.88s/it, loss=0.145, lr=0.0001] Steps:  53%|█████▎    | 267/500 [55:51<14:15,  3.67s/it, loss=0.145, lr=0.0001]Steps:  53%|█████▎    | 267/500 [55:51<14:15,  3.67s/it, loss=0.268, lr=0.0001]Steps:  54%|█████▎    | 268/500 [55:54<13:37,  3.52s/it, loss=0.268, lr=0.0001]Steps:  54%|█████▎    | 268/500 [55:54<13:37,  3.52s/it, loss=0.257, lr=0.0001]Steps:  54%|█████▍    | 269/500 [55:57<13:15,  3.44s/it, loss=0.257, lr=0.0001]Steps:  54%|█████▍    | 269/500 [55:57<13:15,  3.44s/it, loss=0.508, lr=0.0001]Steps:  54%|█████▍    | 270/500 [56:00<12:51,  3.35s/it, loss=0.508, lr=0.0001]Steps:  54%|█████▍    | 270/500 [56:00<12:51,  3.35s/it, loss=0.217, lr=0.0001]Steps:  54%|█████▍    | 271/500 [56:04<12:38,  3.31s/it, loss=0.217, lr=0.0001]Steps:  54%|█████▍    | 271/500 [56:04<12:38,  3.31s/it, loss=0.215, lr=0.0001]Steps:  54%|█████▍    | 272/500 [56:07<12:25,  3.27s/it, loss=0.215, lr=0.0001]Steps:  54%|█████▍    | 272/500 [56:07<12:25,  3.27s/it, loss=0.123, lr=0.0001]Steps:  55%|█████▍    | 273/500 [56:10<12:15,  3.24s/it, loss=0.123, lr=0.0001]Steps:  55%|█████▍    | 273/500 [56:10<12:15,  3.24s/it, loss=0.0827, lr=0.0001]Steps:  55%|█████▍    | 274/500 [56:13<12:08,  3.22s/it, loss=0.0827, lr=0.0001]Steps:  55%|█████▍    | 274/500 [56:13<12:08,  3.22s/it, loss=0.453, lr=0.0001] Steps:  55%|█████▌    | 275/500 [56:16<12:01,  3.20s/it, loss=0.453, lr=0.0001]Steps:  55%|█████▌    | 275/500 [56:16<12:01,  3.20s/it, loss=0.194, lr=0.0001]Steps:  55%|█████▌    | 276/500 [56:19<11:56,  3.20s/it, loss=0.194, lr=0.0001]Steps:  55%|█████▌    | 276/500 [56:20<11:56,  3.20s/it, loss=0.158, lr=0.0001]Steps:  55%|█████▌    | 277/500 [56:23<11:51,  3.19s/it, loss=0.158, lr=0.0001]Steps:  55%|█████▌    | 277/500 [56:23<11:51,  3.19s/it, loss=0.224, lr=0.0001]Steps:  56%|█████▌    | 278/500 [56:26<11:46,  3.18s/it, loss=0.224, lr=0.0001]Steps:  56%|█████▌    | 278/500 [56:26<11:46,  3.18s/it, loss=0.401, lr=0.0001]Steps:  56%|█████▌    | 279/500 [56:29<11:43,  3.18s/it, loss=0.401, lr=0.0001]Steps:  56%|█████▌    | 279/500 [56:29<11:43,  3.18s/it, loss=0.109, lr=0.0001]Steps:  56%|█████▌    | 280/500 [56:32<11:38,  3.17s/it, loss=0.109, lr=0.0001]Steps:  56%|█████▌    | 280/500 [56:32<11:38,  3.17s/it, loss=0.471, lr=0.0001]Steps:  56%|█████▌    | 281/500 [56:35<11:37,  3.18s/it, loss=0.471, lr=0.0001]Steps:  56%|█████▌    | 281/500 [56:35<11:37,  3.18s/it, loss=0.275, lr=0.0001]Steps:  56%|█████▋    | 282/500 [56:39<11:32,  3.18s/it, loss=0.275, lr=0.0001]Steps:  56%|█████▋    | 282/500 [56:39<11:32,  3.18s/it, loss=0.211, lr=0.0001]Steps:  57%|█████▋    | 283/500 [56:42<11:29,  3.18s/it, loss=0.211, lr=0.0001]Steps:  57%|█████▋    | 283/500 [56:42<11:29,  3.18s/it, loss=0.102, lr=0.0001]Steps:  57%|█████▋    | 284/500 [56:45<11:26,  3.18s/it, loss=0.102, lr=0.0001]Steps:  57%|█████▋    | 284/500 [56:45<11:26,  3.18s/it, loss=0.183, lr=0.0001]Steps:  57%|█████▋    | 285/500 [56:48<11:22,  3.18s/it, loss=0.183, lr=0.0001]Steps:  57%|█████▋    | 285/500 [56:48<11:22,  3.18s/it, loss=0.278, lr=0.0001]Steps:  57%|█████▋    | 286/500 [56:51<11:19,  3.17s/it, loss=0.278, lr=0.0001]Steps:  57%|█████▋    | 286/500 [56:51<11:19,  3.17s/it, loss=0.141, lr=0.0001]Steps:  57%|█████▋    | 287/500 [56:54<11:15,  3.17s/it, loss=0.141, lr=0.0001]Steps:  57%|█████▋    | 287/500 [56:54<11:15,  3.17s/it, loss=0.767, lr=0.0001]Steps:  58%|█████▊    | 288/500 [56:58<11:12,  3.17s/it, loss=0.767, lr=0.0001]Steps:  58%|█████▊    | 288/500 [56:58<11:12,  3.17s/it, loss=0.224, lr=0.0001]Steps:  58%|█████▊    | 289/500 [57:01<11:09,  3.17s/it, loss=0.224, lr=0.0001]Steps:  58%|█████▊    | 289/500 [57:01<11:09,  3.17s/it, loss=0.181, lr=0.0001]Steps:  58%|█████▊    | 290/500 [57:04<11:04,  3.17s/it, loss=0.181, lr=0.0001]Steps:  58%|█████▊    | 290/500 [57:04<11:04,  3.17s/it, loss=0.324, lr=0.0001]Steps:  58%|█████▊    | 291/500 [57:07<11:03,  3.18s/it, loss=0.324, lr=0.0001]Steps:  58%|█████▊    | 291/500 [57:07<11:03,  3.18s/it, loss=0.263, lr=0.0001]Steps:  58%|█████▊    | 292/500 [57:10<11:00,  3.17s/it, loss=0.263, lr=0.0001]Steps:  58%|█████▊    | 292/500 [57:10<11:00,  3.17s/it, loss=0.224, lr=0.0001]Steps:  59%|█████▊    | 293/500 [57:13<10:57,  3.17s/it, loss=0.224, lr=0.0001]Steps:  59%|█████▊    | 293/500 [57:14<10:57,  3.17s/it, loss=0.101, lr=0.0001]Steps:  59%|█████▉    | 294/500 [57:17<10:54,  3.18s/it, loss=0.101, lr=0.0001]Steps:  59%|█████▉    | 294/500 [57:17<10:54,  3.18s/it, loss=0.445, lr=0.0001]Steps:  59%|█████▉    | 295/500 [57:20<10:50,  3.17s/it, loss=0.445, lr=0.0001]Steps:  59%|█████▉    | 295/500 [57:20<10:50,  3.17s/it, loss=0.17, lr=0.0001] Steps:  59%|█████▉    | 296/500 [57:23<10:46,  3.17s/it, loss=0.17, lr=0.0001]Steps:  59%|█████▉    | 296/500 [57:23<10:46,  3.17s/it, loss=0.146, lr=0.0001]Steps:  59%|█████▉    | 297/500 [57:26<10:49,  3.20s/it, loss=0.146, lr=0.0001]Steps:  59%|█████▉    | 297/500 [57:26<10:49,  3.20s/it, loss=0.0997, lr=0.0001]Steps:  60%|█████▉    | 298/500 [57:29<10:44,  3.19s/it, loss=0.0997, lr=0.0001]Steps:  60%|█████▉    | 298/500 [57:29<10:44,  3.19s/it, loss=0.219, lr=0.0001] Steps:  60%|█████▉    | 299/500 [57:33<10:40,  3.19s/it, loss=0.219, lr=0.0001]Steps:  60%|█████▉    | 299/500 [57:33<10:40,  3.19s/it, loss=0.256, lr=0.0001]Steps:  60%|██████    | 300/500 [57:36<10:35,  3.18s/it, loss=0.256, lr=0.0001]05/07/2025 01:27:06 - INFO - __main__ - Running validation at step 300
05/07/2025 01:27:06 - INFO - __main__ - Running validation... 
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][AInstantiating AutoencoderKL model under default dtype torch.bfloat16.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at /root/autodl-tmp/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/vae.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
Loaded vae as AutoencoderKL from `vae` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.

Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 17.91it/s][A{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A[ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 70.66it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 31.41it/s]
05/07/2025 01:27:08 - INFO - __main__ - Loaded 6 prompts from validation_prompts.json
05/07/2025 01:27:50 - INFO - __main__ - Generated image 1/2 for prompt 1/6
05/07/2025 01:28:31 - INFO - __main__ - Generated image 2/2 for prompt 1/6
05/07/2025 01:29:12 - INFO - __main__ - Generated image 1/2 for prompt 2/6
05/07/2025 01:29:53 - INFO - __main__ - Generated image 2/2 for prompt 2/6
05/07/2025 01:30:34 - INFO - __main__ - Generated image 1/2 for prompt 3/6
05/07/2025 01:31:16 - INFO - __main__ - Generated image 2/2 for prompt 3/6
05/07/2025 01:31:57 - INFO - __main__ - Generated image 1/2 for prompt 4/6
05/07/2025 01:32:38 - INFO - __main__ - Generated image 2/2 for prompt 4/6
05/07/2025 01:33:20 - INFO - __main__ - Generated image 1/2 for prompt 5/6
05/07/2025 01:34:01 - INFO - __main__ - Generated image 2/2 for prompt 5/6
05/07/2025 01:34:42 - INFO - __main__ - Generated image 1/2 for prompt 6/6
05/07/2025 01:35:24 - INFO - __main__ - Generated image 2/2 for prompt 6/6
Steps:  60%|██████    | 300/500 [1:05:54<10:35,  3.18s/it, loss=0.479, lr=0.0001]Steps:  60%|██████    | 301/500 [1:05:57<8:26:19, 152.66s/it, loss=0.479, lr=0.0001]Steps:  60%|██████    | 301/500 [1:05:57<8:26:19, 152.66s/it, loss=0.493, lr=0.0001]Steps:  60%|██████    | 302/500 [1:06:00<5:55:47, 107.81s/it, loss=0.493, lr=0.0001]Steps:  60%|██████    | 302/500 [1:06:00<5:55:47, 107.81s/it, loss=0.173, lr=0.0001]Steps:  61%|██████    | 303/500 [1:06:04<4:10:54, 76.42s/it, loss=0.173, lr=0.0001] Steps:  61%|██████    | 303/500 [1:06:04<4:10:54, 76.42s/it, loss=0.224, lr=0.0001]Steps:  61%|██████    | 304/500 [1:06:07<2:57:51, 54.45s/it, loss=0.224, lr=0.0001]Steps:  61%|██████    | 304/500 [1:06:07<2:57:51, 54.45s/it, loss=0.116, lr=0.0001]Steps:  61%|██████    | 305/500 [1:06:10<2:06:57, 39.06s/it, loss=0.116, lr=0.0001]Steps:  61%|██████    | 305/500 [1:06:10<2:06:57, 39.06s/it, loss=0.428, lr=0.0001]Steps:  61%|██████    | 306/500 [1:06:13<1:31:29, 28.29s/it, loss=0.428, lr=0.0001]Steps:  61%|██████    | 306/500 [1:06:13<1:31:29, 28.29s/it, loss=0.0959, lr=0.0001]Steps:  61%|██████▏   | 307/500 [1:06:16<1:06:46, 20.76s/it, loss=0.0959, lr=0.0001]Steps:  61%|██████▏   | 307/500 [1:06:16<1:06:46, 20.76s/it, loss=0.151, lr=0.0001] Steps:  62%|██████▏   | 308/500 [1:06:19<49:32, 15.48s/it, loss=0.151, lr=0.0001]  Steps:  62%|██████▏   | 308/500 [1:06:19<49:32, 15.48s/it, loss=0.0942, lr=0.0001]Steps:  62%|██████▏   | 309/500 [1:06:23<37:33, 11.80s/it, loss=0.0942, lr=0.0001]Steps:  62%|██████▏   | 309/500 [1:06:23<37:33, 11.80s/it, loss=1.13, lr=0.0001]  Steps:  62%|██████▏   | 310/500 [1:06:26<29:08,  9.20s/it, loss=1.13, lr=0.0001]Steps:  62%|██████▏   | 310/500 [1:06:26<29:08,  9.20s/it, loss=0.439, lr=0.0001]Steps:  62%|██████▏   | 311/500 [1:06:29<23:19,  7.40s/it, loss=0.439, lr=0.0001]Steps:  62%|██████▏   | 311/500 [1:06:29<23:19,  7.40s/it, loss=0.222, lr=0.0001]Steps:  62%|██████▏   | 312/500 [1:06:32<19:12,  6.13s/it, loss=0.222, lr=0.0001]Steps:  62%|██████▏   | 312/500 [1:06:32<19:12,  6.13s/it, loss=0.132, lr=0.0001]Steps:  63%|██████▎   | 313/500 [1:06:35<16:20,  5.25s/it, loss=0.132, lr=0.0001]Steps:  63%|██████▎   | 313/500 [1:06:35<16:20,  5.25s/it, loss=0.168, lr=0.0001]Steps:  63%|██████▎   | 314/500 [1:06:38<14:20,  4.63s/it, loss=0.168, lr=0.0001]Steps:  63%|██████▎   | 314/500 [1:06:39<14:20,  4.63s/it, loss=0.286, lr=0.0001]Steps:  63%|██████▎   | 315/500 [1:06:42<12:55,  4.19s/it, loss=0.286, lr=0.0001]Steps:  63%|██████▎   | 315/500 [1:06:42<12:55,  4.19s/it, loss=0.209, lr=0.0001]Steps:  63%|██████▎   | 316/500 [1:06:45<11:54,  3.88s/it, loss=0.209, lr=0.0001]Steps:  63%|██████▎   | 316/500 [1:06:45<11:54,  3.88s/it, loss=0.382, lr=0.0001]Steps:  63%|██████▎   | 317/500 [1:06:48<11:11,  3.67s/it, loss=0.382, lr=0.0001]Steps:  63%|██████▎   | 317/500 [1:06:48<11:11,  3.67s/it, loss=0.237, lr=0.0001]Steps:  64%|██████▎   | 318/500 [1:06:51<10:41,  3.52s/it, loss=0.237, lr=0.0001]Steps:  64%|██████▎   | 318/500 [1:06:51<10:41,  3.52s/it, loss=0.0945, lr=0.0001]Steps:  64%|██████▍   | 319/500 [1:06:54<10:22,  3.44s/it, loss=0.0945, lr=0.0001]Steps:  64%|██████▍   | 319/500 [1:06:54<10:22,  3.44s/it, loss=0.448, lr=0.0001] Steps:  64%|██████▍   | 320/500 [1:06:58<10:03,  3.35s/it, loss=0.448, lr=0.0001]Steps:  64%|██████▍   | 320/500 [1:06:58<10:03,  3.35s/it, loss=0.181, lr=0.0001]Steps:  64%|██████▍   | 321/500 [1:07:01<09:51,  3.31s/it, loss=0.181, lr=0.0001]Steps:  64%|██████▍   | 321/500 [1:07:01<09:51,  3.31s/it, loss=0.274, lr=0.0001]Steps:  64%|██████▍   | 322/500 [1:07:04<09:41,  3.27s/it, loss=0.274, lr=0.0001]Steps:  64%|██████▍   | 322/500 [1:07:04<09:41,  3.27s/it, loss=0.17, lr=0.0001] Steps:  65%|██████▍   | 323/500 [1:07:07<09:33,  3.24s/it, loss=0.17, lr=0.0001]Steps:  65%|██████▍   | 323/500 [1:07:07<09:33,  3.24s/it, loss=0.245, lr=0.0001]Steps:  65%|██████▍   | 324/500 [1:07:10<09:26,  3.22s/it, loss=0.245, lr=0.0001]Steps:  65%|██████▍   | 324/500 [1:07:10<09:26,  3.22s/it, loss=0.935, lr=0.0001]Steps:  65%|██████▌   | 325/500 [1:07:13<09:20,  3.20s/it, loss=0.935, lr=0.0001]Steps:  65%|██████▌   | 325/500 [1:07:14<09:20,  3.20s/it, loss=0.215, lr=0.0001]Steps:  65%|██████▌   | 326/500 [1:07:17<09:16,  3.20s/it, loss=0.215, lr=0.0001]Steps:  65%|██████▌   | 326/500 [1:07:17<09:16,  3.20s/it, loss=0.31, lr=0.0001] Steps:  65%|██████▌   | 327/500 [1:07:20<09:11,  3.19s/it, loss=0.31, lr=0.0001]Steps:  65%|██████▌   | 327/500 [1:07:20<09:11,  3.19s/it, loss=0.333, lr=0.0001]Steps:  66%|██████▌   | 328/500 [1:07:23<09:07,  3.18s/it, loss=0.333, lr=0.0001]Steps:  66%|██████▌   | 328/500 [1:07:23<09:07,  3.18s/it, loss=0.242, lr=0.0001]Steps:  66%|██████▌   | 329/500 [1:07:26<09:03,  3.18s/it, loss=0.242, lr=0.0001]Steps:  66%|██████▌   | 329/500 [1:07:26<09:03,  3.18s/it, loss=0.155, lr=0.0001]Steps:  66%|██████▌   | 330/500 [1:07:29<08:58,  3.17s/it, loss=0.155, lr=0.0001]Steps:  66%|██████▌   | 330/500 [1:07:29<08:58,  3.17s/it, loss=0.496, lr=0.0001]Steps:  66%|██████▌   | 331/500 [1:07:32<08:57,  3.18s/it, loss=0.496, lr=0.0001]Steps:  66%|██████▌   | 331/500 [1:07:33<08:57,  3.18s/it, loss=0.354, lr=0.0001]Steps:  66%|██████▋   | 332/500 [1:07:36<08:53,  3.18s/it, loss=0.354, lr=0.0001]Steps:  66%|██████▋   | 332/500 [1:07:36<08:53,  3.18s/it, loss=0.394, lr=0.0001]Steps:  67%|██████▋   | 333/500 [1:07:39<08:50,  3.18s/it, loss=0.394, lr=0.0001]Steps:  67%|██████▋   | 333/500 [1:07:39<08:50,  3.18s/it, loss=0.168, lr=0.0001]Steps:  67%|██████▋   | 334/500 [1:07:42<08:46,  3.17s/it, loss=0.168, lr=0.0001]Steps:  67%|██████▋   | 334/500 [1:07:42<08:46,  3.17s/it, loss=0.223, lr=0.0001]Steps:  67%|██████▋   | 335/500 [1:07:45<08:43,  3.17s/it, loss=0.223, lr=0.0001]Steps:  67%|██████▋   | 335/500 [1:07:45<08:43,  3.17s/it, loss=0.207, lr=0.0001]Steps:  67%|██████▋   | 336/500 [1:07:48<08:40,  3.17s/it, loss=0.207, lr=0.0001]Steps:  67%|██████▋   | 336/500 [1:07:48<08:40,  3.17s/it, loss=0.279, lr=0.0001]Steps:  67%|██████▋   | 337/500 [1:07:51<08:37,  3.17s/it, loss=0.279, lr=0.0001]Steps:  67%|██████▋   | 337/500 [1:07:52<08:37,  3.17s/it, loss=0.459, lr=0.0001]Steps:  68%|██████▊   | 338/500 [1:07:55<08:34,  3.18s/it, loss=0.459, lr=0.0001]Steps:  68%|██████▊   | 338/500 [1:07:55<08:34,  3.18s/it, loss=0.443, lr=0.0001]Steps:  68%|██████▊   | 339/500 [1:07:58<08:31,  3.18s/it, loss=0.443, lr=0.0001]Steps:  68%|██████▊   | 339/500 [1:07:58<08:31,  3.18s/it, loss=0.493, lr=0.0001]Steps:  68%|██████▊   | 340/500 [1:08:01<08:27,  3.17s/it, loss=0.493, lr=0.0001]Steps:  68%|██████▊   | 340/500 [1:08:01<08:27,  3.17s/it, loss=0.144, lr=0.0001]Steps:  68%|██████▊   | 341/500 [1:08:04<08:25,  3.18s/it, loss=0.144, lr=0.0001]Steps:  68%|██████▊   | 341/500 [1:08:04<08:25,  3.18s/it, loss=0.197, lr=0.0001]Steps:  68%|██████▊   | 342/500 [1:08:07<08:22,  3.18s/it, loss=0.197, lr=0.0001]Steps:  68%|██████▊   | 342/500 [1:08:07<08:22,  3.18s/it, loss=0.217, lr=0.0001]Steps:  69%|██████▊   | 343/500 [1:08:11<08:18,  3.18s/it, loss=0.217, lr=0.0001]Steps:  69%|██████▊   | 343/500 [1:08:11<08:18,  3.18s/it, loss=0.509, lr=0.0001]Steps:  69%|██████▉   | 344/500 [1:08:14<08:14,  3.17s/it, loss=0.509, lr=0.0001]Steps:  69%|██████▉   | 344/500 [1:08:14<08:14,  3.17s/it, loss=0.117, lr=0.0001]Steps:  69%|██████▉   | 345/500 [1:08:17<08:11,  3.17s/it, loss=0.117, lr=0.0001]Steps:  69%|██████▉   | 345/500 [1:08:17<08:11,  3.17s/it, loss=0.0844, lr=0.0001]Steps:  69%|██████▉   | 346/500 [1:08:20<08:08,  3.17s/it, loss=0.0844, lr=0.0001]Steps:  69%|██████▉   | 346/500 [1:08:20<08:08,  3.17s/it, loss=0.233, lr=0.0001] Steps:  69%|██████▉   | 347/500 [1:08:23<08:10,  3.21s/it, loss=0.233, lr=0.0001]Steps:  69%|██████▉   | 347/500 [1:08:23<08:10,  3.21s/it, loss=0.652, lr=0.0001]Steps:  70%|██████▉   | 348/500 [1:08:27<08:05,  3.19s/it, loss=0.652, lr=0.0001]Steps:  70%|██████▉   | 348/500 [1:08:27<08:05,  3.19s/it, loss=0.419, lr=0.0001]Steps:  70%|██████▉   | 349/500 [1:08:30<08:01,  3.19s/it, loss=0.419, lr=0.0001]Steps:  70%|██████▉   | 349/500 [1:08:30<08:01,  3.19s/it, loss=0.0973, lr=0.0001]Steps:  70%|███████   | 350/500 [1:08:33<07:56,  3.18s/it, loss=0.0973, lr=0.0001]05/07/2025 01:38:03 - INFO - __main__ - Running validation at step 350
05/07/2025 01:38:03 - INFO - __main__ - Running validation... 
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][AInstantiating AutoencoderKL model under default dtype torch.bfloat16.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at /root/autodl-tmp/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/vae.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
Loaded vae as AutoencoderKL from `vae` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.

Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 16.93it/s][A{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A[ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 70.53it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 29.97it/s]
05/07/2025 01:38:06 - INFO - __main__ - Loaded 6 prompts from validation_prompts.json
05/07/2025 01:38:47 - INFO - __main__ - Generated image 1/2 for prompt 1/6
05/07/2025 01:39:28 - INFO - __main__ - Generated image 2/2 for prompt 1/6
05/07/2025 01:40:09 - INFO - __main__ - Generated image 1/2 for prompt 2/6
05/07/2025 01:40:50 - INFO - __main__ - Generated image 2/2 for prompt 2/6
05/07/2025 01:41:32 - INFO - __main__ - Generated image 1/2 for prompt 3/6
05/07/2025 01:42:13 - INFO - __main__ - Generated image 2/2 for prompt 3/6
05/07/2025 01:42:54 - INFO - __main__ - Generated image 1/2 for prompt 4/6
05/07/2025 01:43:36 - INFO - __main__ - Generated image 2/2 for prompt 4/6
05/07/2025 01:44:17 - INFO - __main__ - Generated image 1/2 for prompt 5/6
05/07/2025 01:44:59 - INFO - __main__ - Generated image 2/2 for prompt 5/6
05/07/2025 01:45:40 - INFO - __main__ - Generated image 1/2 for prompt 6/6
05/07/2025 01:46:21 - INFO - __main__ - Generated image 2/2 for prompt 6/6
Steps:  70%|███████   | 350/500 [1:16:52<07:56,  3.18s/it, loss=0.183, lr=0.0001] Steps:  70%|███████   | 351/500 [1:16:55<6:19:29, 152.81s/it, loss=0.183, lr=0.0001]Steps:  70%|███████   | 351/500 [1:16:55<6:19:29, 152.81s/it, loss=0.191, lr=0.0001]Steps:  70%|███████   | 352/500 [1:16:58<4:26:11, 107.92s/it, loss=0.191, lr=0.0001]Steps:  70%|███████   | 352/500 [1:16:58<4:26:11, 107.92s/it, loss=0.455, lr=0.0001]Steps:  71%|███████   | 353/500 [1:17:01<3:07:24, 76.49s/it, loss=0.455, lr=0.0001] Steps:  71%|███████   | 353/500 [1:17:01<3:07:24, 76.49s/it, loss=0.35, lr=0.0001] Steps:  71%|███████   | 354/500 [1:17:04<2:12:36, 54.50s/it, loss=0.35, lr=0.0001]Steps:  71%|███████   | 354/500 [1:17:04<2:12:36, 54.50s/it, loss=0.131, lr=0.0001]Steps:  71%|███████   | 355/500 [1:17:07<1:34:29, 39.10s/it, loss=0.131, lr=0.0001]Steps:  71%|███████   | 355/500 [1:17:08<1:34:29, 39.10s/it, loss=0.479, lr=0.0001]Steps:  71%|███████   | 356/500 [1:17:11<1:07:58, 28.32s/it, loss=0.479, lr=0.0001]Steps:  71%|███████   | 356/500 [1:17:11<1:07:58, 28.32s/it, loss=0.191, lr=0.0001]Steps:  71%|███████▏  | 357/500 [1:17:14<49:31, 20.78s/it, loss=0.191, lr=0.0001]  Steps:  71%|███████▏  | 357/500 [1:17:14<49:31, 20.78s/it, loss=0.233, lr=0.0001]Steps:  72%|███████▏  | 358/500 [1:17:17<36:40, 15.50s/it, loss=0.233, lr=0.0001]Steps:  72%|███████▏  | 358/500 [1:17:17<36:40, 15.50s/it, loss=0.487, lr=0.0001]Steps:  72%|███████▏  | 359/500 [1:17:20<27:43, 11.80s/it, loss=0.487, lr=0.0001]Steps:  72%|███████▏  | 359/500 [1:17:20<27:43, 11.80s/it, loss=0.489, lr=0.0001]Steps:  72%|███████▏  | 360/500 [1:17:23<21:28,  9.20s/it, loss=0.489, lr=0.0001]Steps:  72%|███████▏  | 360/500 [1:17:23<21:28,  9.20s/it, loss=0.0982, lr=0.0001]Steps:  72%|███████▏  | 361/500 [1:17:27<17:09,  7.40s/it, loss=0.0982, lr=0.0001]Steps:  72%|███████▏  | 361/500 [1:17:27<17:09,  7.40s/it, loss=0.0892, lr=0.0001]Steps:  72%|███████▏  | 362/500 [1:17:30<14:07,  6.14s/it, loss=0.0892, lr=0.0001]Steps:  72%|███████▏  | 362/500 [1:17:30<14:07,  6.14s/it, loss=0.264, lr=0.0001] Steps:  73%|███████▎  | 363/500 [1:17:33<11:59,  5.25s/it, loss=0.264, lr=0.0001]Steps:  73%|███████▎  | 363/500 [1:17:33<11:59,  5.25s/it, loss=0.147, lr=0.0001]Steps:  73%|███████▎  | 364/500 [1:17:36<10:29,  4.63s/it, loss=0.147, lr=0.0001]Steps:  73%|███████▎  | 364/500 [1:17:36<10:29,  4.63s/it, loss=0.169, lr=0.0001]Steps:  73%|███████▎  | 365/500 [1:17:39<09:25,  4.19s/it, loss=0.169, lr=0.0001]Steps:  73%|███████▎  | 365/500 [1:17:39<09:25,  4.19s/it, loss=0.633, lr=0.0001]Steps:  73%|███████▎  | 366/500 [1:17:42<08:40,  3.88s/it, loss=0.633, lr=0.0001]Steps:  73%|███████▎  | 366/500 [1:17:42<08:40,  3.88s/it, loss=0.0828, lr=0.0001]Steps:  73%|███████▎  | 367/500 [1:17:46<08:08,  3.67s/it, loss=0.0828, lr=0.0001]Steps:  73%|███████▎  | 367/500 [1:17:46<08:08,  3.67s/it, loss=0.172, lr=0.0001] Steps:  74%|███████▎  | 368/500 [1:17:49<07:45,  3.52s/it, loss=0.172, lr=0.0001]Steps:  74%|███████▎  | 368/500 [1:17:49<07:45,  3.52s/it, loss=0.224, lr=0.0001]Steps:  74%|███████▍  | 369/500 [1:17:52<07:30,  3.44s/it, loss=0.224, lr=0.0001]Steps:  74%|███████▍  | 369/500 [1:17:52<07:30,  3.44s/it, loss=0.389, lr=0.0001]Steps:  74%|███████▍  | 370/500 [1:17:55<07:16,  3.35s/it, loss=0.389, lr=0.0001]Steps:  74%|███████▍  | 370/500 [1:17:55<07:16,  3.35s/it, loss=0.477, lr=0.0001]Steps:  74%|███████▍  | 371/500 [1:17:58<07:06,  3.31s/it, loss=0.477, lr=0.0001]Steps:  74%|███████▍  | 371/500 [1:17:58<07:06,  3.31s/it, loss=0.183, lr=0.0001]Steps:  74%|███████▍  | 372/500 [1:18:02<06:58,  3.27s/it, loss=0.183, lr=0.0001]Steps:  74%|███████▍  | 372/500 [1:18:02<06:58,  3.27s/it, loss=0.26, lr=0.0001] Steps:  75%|███████▍  | 373/500 [1:18:05<06:51,  3.24s/it, loss=0.26, lr=0.0001]Steps:  75%|███████▍  | 373/500 [1:18:05<06:51,  3.24s/it, loss=0.223, lr=0.0001]Steps:  75%|███████▍  | 374/500 [1:18:08<06:45,  3.22s/it, loss=0.223, lr=0.0001]Steps:  75%|███████▍  | 374/500 [1:18:08<06:45,  3.22s/it, loss=0.901, lr=0.0001]Steps:  75%|███████▌  | 375/500 [1:18:11<06:40,  3.20s/it, loss=0.901, lr=0.0001]Steps:  75%|███████▌  | 375/500 [1:18:11<06:40,  3.20s/it, loss=0.194, lr=0.0001]Steps:  75%|███████▌  | 376/500 [1:18:14<06:35,  3.19s/it, loss=0.194, lr=0.0001]Steps:  75%|███████▌  | 376/500 [1:18:14<06:35,  3.19s/it, loss=0.106, lr=0.0001]Steps:  75%|███████▌  | 377/500 [1:18:17<06:32,  3.19s/it, loss=0.106, lr=0.0001]Steps:  75%|███████▌  | 377/500 [1:18:18<06:32,  3.19s/it, loss=0.0838, lr=0.0001]Steps:  76%|███████▌  | 378/500 [1:18:21<06:28,  3.18s/it, loss=0.0838, lr=0.0001]Steps:  76%|███████▌  | 378/500 [1:18:21<06:28,  3.18s/it, loss=0.457, lr=0.0001] Steps:  76%|███████▌  | 379/500 [1:18:24<06:24,  3.18s/it, loss=0.457, lr=0.0001]Steps:  76%|███████▌  | 379/500 [1:18:24<06:24,  3.18s/it, loss=0.117, lr=0.0001]Steps:  76%|███████▌  | 380/500 [1:18:27<06:20,  3.17s/it, loss=0.117, lr=0.0001]Steps:  76%|███████▌  | 380/500 [1:18:27<06:20,  3.17s/it, loss=0.306, lr=0.0001]Steps:  76%|███████▌  | 381/500 [1:18:30<06:18,  3.18s/it, loss=0.306, lr=0.0001]Steps:  76%|███████▌  | 381/500 [1:18:30<06:18,  3.18s/it, loss=0.101, lr=0.0001]Steps:  76%|███████▋  | 382/500 [1:18:33<06:14,  3.17s/it, loss=0.101, lr=0.0001]Steps:  76%|███████▋  | 382/500 [1:18:33<06:14,  3.17s/it, loss=0.0901, lr=0.0001]Steps:  77%|███████▋  | 383/500 [1:18:36<06:11,  3.18s/it, loss=0.0901, lr=0.0001]Steps:  77%|███████▋  | 383/500 [1:18:37<06:11,  3.18s/it, loss=0.0972, lr=0.0001]Steps:  77%|███████▋  | 384/500 [1:18:40<06:08,  3.18s/it, loss=0.0972, lr=0.0001]Steps:  77%|███████▋  | 384/500 [1:18:40<06:08,  3.18s/it, loss=0.313, lr=0.0001] Steps:  77%|███████▋  | 385/500 [1:18:43<06:05,  3.17s/it, loss=0.313, lr=0.0001]Steps:  77%|███████▋  | 385/500 [1:18:43<06:05,  3.17s/it, loss=0.43, lr=0.0001] Steps:  77%|███████▋  | 386/500 [1:18:46<06:01,  3.17s/it, loss=0.43, lr=0.0001]Steps:  77%|███████▋  | 386/500 [1:18:46<06:01,  3.17s/it, loss=0.0846, lr=0.0001]Steps:  77%|███████▋  | 387/500 [1:18:49<05:58,  3.18s/it, loss=0.0846, lr=0.0001]Steps:  77%|███████▋  | 387/500 [1:18:49<05:58,  3.18s/it, loss=0.29, lr=0.0001]  Steps:  78%|███████▊  | 388/500 [1:18:52<05:55,  3.17s/it, loss=0.29, lr=0.0001]Steps:  78%|███████▊  | 388/500 [1:18:52<05:55,  3.17s/it, loss=0.487, lr=0.0001]Steps:  78%|███████▊  | 389/500 [1:18:55<05:52,  3.17s/it, loss=0.487, lr=0.0001]Steps:  78%|███████▊  | 389/500 [1:18:56<05:52,  3.17s/it, loss=0.109, lr=0.0001]Steps:  78%|███████▊  | 390/500 [1:18:59<05:48,  3.17s/it, loss=0.109, lr=0.0001]Steps:  78%|███████▊  | 390/500 [1:18:59<05:48,  3.17s/it, loss=0.379, lr=0.0001]Steps:  78%|███████▊  | 391/500 [1:19:02<05:46,  3.18s/it, loss=0.379, lr=0.0001]Steps:  78%|███████▊  | 391/500 [1:19:02<05:46,  3.18s/it, loss=0.263, lr=0.0001]Steps:  78%|███████▊  | 392/500 [1:19:05<05:43,  3.18s/it, loss=0.263, lr=0.0001]Steps:  78%|███████▊  | 392/500 [1:19:05<05:43,  3.18s/it, loss=0.208, lr=0.0001]Steps:  79%|███████▊  | 393/500 [1:19:08<05:39,  3.17s/it, loss=0.208, lr=0.0001]Steps:  79%|███████▊  | 393/500 [1:19:08<05:39,  3.17s/it, loss=0.109, lr=0.0001]Steps:  79%|███████▉  | 394/500 [1:19:11<05:36,  3.17s/it, loss=0.109, lr=0.0001]Steps:  79%|███████▉  | 394/500 [1:19:11<05:36,  3.17s/it, loss=0.0903, lr=0.0001]Steps:  79%|███████▉  | 395/500 [1:19:15<05:33,  3.17s/it, loss=0.0903, lr=0.0001]Steps:  79%|███████▉  | 395/500 [1:19:15<05:33,  3.17s/it, loss=0.0846, lr=0.0001]Steps:  79%|███████▉  | 396/500 [1:19:18<05:30,  3.17s/it, loss=0.0846, lr=0.0001]Steps:  79%|███████▉  | 396/500 [1:19:18<05:30,  3.17s/it, loss=0.307, lr=0.0001] Steps:  79%|███████▉  | 397/500 [1:19:21<05:29,  3.20s/it, loss=0.307, lr=0.0001]Steps:  79%|███████▉  | 397/500 [1:19:21<05:29,  3.20s/it, loss=0.726, lr=0.0001]Steps:  80%|███████▉  | 398/500 [1:19:24<05:25,  3.19s/it, loss=0.726, lr=0.0001]Steps:  80%|███████▉  | 398/500 [1:19:24<05:25,  3.19s/it, loss=0.482, lr=0.0001]Steps:  80%|███████▉  | 399/500 [1:19:27<05:21,  3.19s/it, loss=0.482, lr=0.0001]Steps:  80%|███████▉  | 399/500 [1:19:27<05:21,  3.19s/it, loss=0.398, lr=0.0001]Steps:  80%|████████  | 400/500 [1:19:30<05:17,  3.18s/it, loss=0.398, lr=0.0001]05/07/2025 01:49:00 - INFO - __main__ - Running validation at step 400
05/07/2025 01:49:00 - INFO - __main__ - Running validation... 
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][AInstantiating AutoencoderKL model under default dtype torch.bfloat16.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at /root/autodl-tmp/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/vae.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
Loaded vae as AutoencoderKL from `vae` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.

Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 17.81it/s][A{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A[ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 71.21it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 31.32it/s]
05/07/2025 01:49:03 - INFO - __main__ - Loaded 6 prompts from validation_prompts.json
05/07/2025 01:49:44 - INFO - __main__ - Generated image 1/2 for prompt 1/6
05/07/2025 01:50:25 - INFO - __main__ - Generated image 2/2 for prompt 1/6
05/07/2025 01:51:07 - INFO - __main__ - Generated image 1/2 for prompt 2/6
05/07/2025 01:51:48 - INFO - __main__ - Generated image 2/2 for prompt 2/6
05/07/2025 01:52:29 - INFO - __main__ - Generated image 1/2 for prompt 3/6
05/07/2025 01:53:11 - INFO - __main__ - Generated image 2/2 for prompt 3/6
05/07/2025 01:53:52 - INFO - __main__ - Generated image 1/2 for prompt 4/6
05/07/2025 01:54:33 - INFO - __main__ - Generated image 2/2 for prompt 4/6
05/07/2025 01:55:15 - INFO - __main__ - Generated image 1/2 for prompt 5/6
05/07/2025 01:55:56 - INFO - __main__ - Generated image 2/2 for prompt 5/6
05/07/2025 01:56:37 - INFO - __main__ - Generated image 1/2 for prompt 6/6
05/07/2025 01:57:19 - INFO - __main__ - Generated image 2/2 for prompt 6/6
Steps:  80%|████████  | 400/500 [1:27:49<05:17,  3.18s/it, loss=0.0963, lr=0.0001]Steps:  80%|████████  | 401/500 [1:27:52<4:12:04, 152.77s/it, loss=0.0963, lr=0.0001]Steps:  80%|████████  | 401/500 [1:27:52<4:12:04, 152.77s/it, loss=0.307, lr=0.0001] Steps:  80%|████████  | 402/500 [1:27:55<2:56:13, 107.89s/it, loss=0.307, lr=0.0001]Steps:  80%|████████  | 402/500 [1:27:56<2:56:13, 107.89s/it, loss=0.0795, lr=0.0001]Steps:  81%|████████  | 403/500 [1:27:59<2:03:38, 76.47s/it, loss=0.0795, lr=0.0001] Steps:  81%|████████  | 403/500 [1:27:59<2:03:38, 76.47s/it, loss=0.305, lr=0.0001] Steps:  81%|████████  | 404/500 [1:28:02<1:27:10, 54.48s/it, loss=0.305, lr=0.0001]Steps:  81%|████████  | 404/500 [1:28:02<1:27:10, 54.48s/it, loss=0.206, lr=0.0001]Steps:  81%|████████  | 405/500 [1:28:05<1:01:53, 39.09s/it, loss=0.206, lr=0.0001]Steps:  81%|████████  | 405/500 [1:28:05<1:01:53, 39.09s/it, loss=0.474, lr=0.0001]Steps:  81%|████████  | 406/500 [1:28:08<44:21, 28.32s/it, loss=0.474, lr=0.0001]  Steps:  81%|████████  | 406/500 [1:28:08<44:21, 28.32s/it, loss=0.0845, lr=0.0001]Steps:  81%|████████▏ | 407/500 [1:28:11<32:11, 20.77s/it, loss=0.0845, lr=0.0001]Steps:  81%|████████▏ | 407/500 [1:28:11<32:11, 20.77s/it, loss=0.453, lr=0.0001] Steps:  82%|████████▏ | 408/500 [1:28:14<23:45, 15.49s/it, loss=0.453, lr=0.0001]Steps:  82%|████████▏ | 408/500 [1:28:15<23:45, 15.49s/it, loss=0.157, lr=0.0001]Steps:  82%|████████▏ | 409/500 [1:28:18<17:53, 11.79s/it, loss=0.157, lr=0.0001]Steps:  82%|████████▏ | 409/500 [1:28:18<17:53, 11.79s/it, loss=0.373, lr=0.0001]Steps:  82%|████████▏ | 410/500 [1:28:21<13:48,  9.20s/it, loss=0.373, lr=0.0001]Steps:  82%|████████▏ | 410/500 [1:28:21<13:48,  9.20s/it, loss=0.158, lr=0.0001]Steps:  82%|████████▏ | 411/500 [1:28:24<10:58,  7.40s/it, loss=0.158, lr=0.0001]Steps:  82%|████████▏ | 411/500 [1:28:24<10:58,  7.40s/it, loss=0.274, lr=0.0001]Steps:  82%|████████▏ | 412/500 [1:28:27<09:00,  6.14s/it, loss=0.274, lr=0.0001]Steps:  82%|████████▏ | 412/500 [1:28:27<09:00,  6.14s/it, loss=0.943, lr=0.0001]Steps:  83%|████████▎ | 413/500 [1:28:30<07:36,  5.25s/it, loss=0.943, lr=0.0001]Steps:  83%|████████▎ | 413/500 [1:28:30<07:36,  5.25s/it, loss=0.706, lr=0.0001]Steps:  83%|████████▎ | 414/500 [1:28:34<06:37,  4.62s/it, loss=0.706, lr=0.0001]Steps:  83%|████████▎ | 414/500 [1:28:34<06:37,  4.62s/it, loss=0.204, lr=0.0001]Steps:  83%|████████▎ | 415/500 [1:28:37<05:56,  4.19s/it, loss=0.204, lr=0.0001]Steps:  83%|████████▎ | 415/500 [1:28:37<05:56,  4.19s/it, loss=0.0873, lr=0.0001]Steps:  83%|████████▎ | 416/500 [1:28:40<05:26,  3.89s/it, loss=0.0873, lr=0.0001]Steps:  83%|████████▎ | 416/500 [1:28:40<05:26,  3.89s/it, loss=0.275, lr=0.0001] Steps:  83%|████████▎ | 417/500 [1:28:43<05:04,  3.67s/it, loss=0.275, lr=0.0001]Steps:  83%|████████▎ | 417/500 [1:28:43<05:04,  3.67s/it, loss=0.492, lr=0.0001]Steps:  84%|████████▎ | 418/500 [1:28:46<04:48,  3.52s/it, loss=0.492, lr=0.0001]Steps:  84%|████████▎ | 418/500 [1:28:46<04:48,  3.52s/it, loss=0.444, lr=0.0001]Steps:  84%|████████▍ | 419/500 [1:28:49<04:38,  3.44s/it, loss=0.444, lr=0.0001]Steps:  84%|████████▍ | 419/500 [1:28:50<04:38,  3.44s/it, loss=0.0897, lr=0.0001]Steps:  84%|████████▍ | 420/500 [1:28:53<04:28,  3.35s/it, loss=0.0897, lr=0.0001]Steps:  84%|████████▍ | 420/500 [1:28:53<04:28,  3.35s/it, loss=0.161, lr=0.0001] Steps:  84%|████████▍ | 421/500 [1:28:56<04:21,  3.30s/it, loss=0.161, lr=0.0001]Steps:  84%|████████▍ | 421/500 [1:28:56<04:21,  3.30s/it, loss=0.174, lr=0.0001]Steps:  84%|████████▍ | 422/500 [1:28:59<04:14,  3.27s/it, loss=0.174, lr=0.0001]Steps:  84%|████████▍ | 422/500 [1:28:59<04:14,  3.27s/it, loss=0.174, lr=0.0001]Steps:  85%|████████▍ | 423/500 [1:29:02<04:09,  3.24s/it, loss=0.174, lr=0.0001]Steps:  85%|████████▍ | 423/500 [1:29:02<04:09,  3.24s/it, loss=0.258, lr=0.0001]Steps:  85%|████████▍ | 424/500 [1:29:05<04:04,  3.22s/it, loss=0.258, lr=0.0001]Steps:  85%|████████▍ | 424/500 [1:29:05<04:04,  3.22s/it, loss=0.0788, lr=0.0001]Steps:  85%|████████▌ | 425/500 [1:29:09<04:00,  3.20s/it, loss=0.0788, lr=0.0001]Steps:  85%|████████▌ | 425/500 [1:29:09<04:00,  3.20s/it, loss=0.403, lr=0.0001] Steps:  85%|████████▌ | 426/500 [1:29:12<03:56,  3.19s/it, loss=0.403, lr=0.0001]Steps:  85%|████████▌ | 426/500 [1:29:12<03:56,  3.19s/it, loss=0.257, lr=0.0001]Steps:  85%|████████▌ | 427/500 [1:29:15<03:52,  3.19s/it, loss=0.257, lr=0.0001]Steps:  85%|████████▌ | 427/500 [1:29:15<03:52,  3.19s/it, loss=0.104, lr=0.0001]Steps:  86%|████████▌ | 428/500 [1:29:18<03:49,  3.18s/it, loss=0.104, lr=0.0001]Steps:  86%|████████▌ | 428/500 [1:29:18<03:49,  3.18s/it, loss=0.492, lr=0.0001]Steps:  86%|████████▌ | 429/500 [1:29:21<03:46,  3.18s/it, loss=0.492, lr=0.0001]Steps:  86%|████████▌ | 429/500 [1:29:21<03:46,  3.18s/it, loss=0.345, lr=0.0001]Steps:  86%|████████▌ | 430/500 [1:29:24<03:42,  3.17s/it, loss=0.345, lr=0.0001]Steps:  86%|████████▌ | 430/500 [1:29:24<03:42,  3.17s/it, loss=0.23, lr=0.0001] Steps:  86%|████████▌ | 431/500 [1:29:28<03:39,  3.18s/it, loss=0.23, lr=0.0001]Steps:  86%|████████▌ | 431/500 [1:29:28<03:39,  3.18s/it, loss=0.303, lr=0.0001]Steps:  86%|████████▋ | 432/500 [1:29:31<03:36,  3.18s/it, loss=0.303, lr=0.0001]Steps:  86%|████████▋ | 432/500 [1:29:31<03:36,  3.18s/it, loss=0.229, lr=0.0001]Steps:  87%|████████▋ | 433/500 [1:29:34<03:32,  3.17s/it, loss=0.229, lr=0.0001]Steps:  87%|████████▋ | 433/500 [1:29:34<03:32,  3.17s/it, loss=0.112, lr=0.0001]Steps:  87%|████████▋ | 434/500 [1:29:37<03:29,  3.18s/it, loss=0.112, lr=0.0001]Steps:  87%|████████▋ | 434/500 [1:29:37<03:29,  3.18s/it, loss=0.185, lr=0.0001]Steps:  87%|████████▋ | 435/500 [1:29:40<03:26,  3.17s/it, loss=0.185, lr=0.0001]Steps:  87%|████████▋ | 435/500 [1:29:40<03:26,  3.17s/it, loss=0.312, lr=0.0001]Steps:  87%|████████▋ | 436/500 [1:29:43<03:23,  3.17s/it, loss=0.312, lr=0.0001]Steps:  87%|████████▋ | 436/500 [1:29:44<03:23,  3.17s/it, loss=0.315, lr=0.0001]Steps:  87%|████████▋ | 437/500 [1:29:47<03:19,  3.17s/it, loss=0.315, lr=0.0001]Steps:  87%|████████▋ | 437/500 [1:29:47<03:19,  3.17s/it, loss=0.283, lr=0.0001]Steps:  88%|████████▊ | 438/500 [1:29:50<03:16,  3.17s/it, loss=0.283, lr=0.0001]Steps:  88%|████████▊ | 438/500 [1:29:50<03:16,  3.17s/it, loss=0.185, lr=0.0001]Steps:  88%|████████▊ | 439/500 [1:29:53<03:13,  3.18s/it, loss=0.185, lr=0.0001]Steps:  88%|████████▊ | 439/500 [1:29:53<03:13,  3.18s/it, loss=0.456, lr=0.0001]Steps:  88%|████████▊ | 440/500 [1:29:56<03:10,  3.17s/it, loss=0.456, lr=0.0001]Steps:  88%|████████▊ | 440/500 [1:29:56<03:10,  3.17s/it, loss=0.274, lr=0.0001]Steps:  88%|████████▊ | 441/500 [1:29:59<03:08,  3.19s/it, loss=0.274, lr=0.0001]Steps:  88%|████████▊ | 441/500 [1:29:59<03:08,  3.19s/it, loss=0.176, lr=0.0001]Steps:  88%|████████▊ | 442/500 [1:30:03<03:04,  3.18s/it, loss=0.176, lr=0.0001]Steps:  88%|████████▊ | 442/500 [1:30:03<03:04,  3.18s/it, loss=0.148, lr=0.0001]Steps:  89%|████████▊ | 443/500 [1:30:06<03:01,  3.18s/it, loss=0.148, lr=0.0001]Steps:  89%|████████▊ | 443/500 [1:30:06<03:01,  3.18s/it, loss=0.0844, lr=0.0001]Steps:  89%|████████▉ | 444/500 [1:30:09<02:58,  3.18s/it, loss=0.0844, lr=0.0001]Steps:  89%|████████▉ | 444/500 [1:30:09<02:58,  3.18s/it, loss=0.133, lr=0.0001] Steps:  89%|████████▉ | 445/500 [1:30:12<02:54,  3.18s/it, loss=0.133, lr=0.0001]Steps:  89%|████████▉ | 445/500 [1:30:12<02:54,  3.18s/it, loss=0.152, lr=0.0001]Steps:  89%|████████▉ | 446/500 [1:30:15<02:51,  3.18s/it, loss=0.152, lr=0.0001]Steps:  89%|████████▉ | 446/500 [1:30:15<02:51,  3.18s/it, loss=0.221, lr=0.0001]Steps:  89%|████████▉ | 447/500 [1:30:18<02:49,  3.20s/it, loss=0.221, lr=0.0001]Steps:  89%|████████▉ | 447/500 [1:30:19<02:49,  3.20s/it, loss=0.226, lr=0.0001]Steps:  90%|████████▉ | 448/500 [1:30:22<02:46,  3.20s/it, loss=0.226, lr=0.0001]Steps:  90%|████████▉ | 448/500 [1:30:22<02:46,  3.20s/it, loss=0.397, lr=0.0001]Steps:  90%|████████▉ | 449/500 [1:30:25<02:42,  3.19s/it, loss=0.397, lr=0.0001]Steps:  90%|████████▉ | 449/500 [1:30:25<02:42,  3.19s/it, loss=0.231, lr=0.0001]Steps:  90%|█████████ | 450/500 [1:30:28<02:38,  3.18s/it, loss=0.231, lr=0.0001]05/07/2025 01:59:58 - INFO - __main__ - Running validation at step 450
05/07/2025 01:59:58 - INFO - __main__ - Running validation... 
{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][AInstantiating AutoencoderKL model under default dtype torch.bfloat16.
All model checkpoint weights were used when initializing AutoencoderKL.

All the weights of AutoencoderKL were initialized from the model checkpoint at /root/autodl-tmp/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/vae.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.
Loaded vae as AutoencoderKL from `vae` subfolder of black-forest-labs/FLUX.1-dev.
Loaded tokenizer_2 as T5TokenizerFast from `tokenizer_2` subfolder of black-forest-labs/FLUX.1-dev.

Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 16.04it/s][A{'use_exponential_sigmas', 'stochastic_sampling', 'use_karras_sigmas', 'use_beta_sigmas', 'invert_sigmas', 'time_shift_type', 'shift_terminal'} was not found in config. Values will be initialized to default values.
Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of black-forest-labs/FLUX.1-dev.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A[ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 65.65it/s]
Loaded text_encoder_2 as T5EncoderModel from `text_encoder_2` subfolder of black-forest-labs/FLUX.1-dev.
Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 28.54it/s]
05/07/2025 02:00:01 - INFO - __main__ - Loaded 6 prompts from validation_prompts.json
